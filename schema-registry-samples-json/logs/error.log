2024-07-03 17:08:26 [ INFO] - org.apache.kafka.clients.admin.AdminClientConfig -AbstractConfig.java(370) -AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = java-admin-client
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

  2024-07-03 17:08:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:08:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:08:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019306613
  2024-07-03 17:08:27 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-07-03 17:08:27 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-07-03 17:08:27 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-1] Instantiated an idempotent producer.
  2024-07-03 17:08:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:08:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:08:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019307680
  2024-07-03 17:08:27 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {yang.tests=LEADER_NOT_AVAILABLE}
  2024-07-03 17:08:27 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-1] Cluster ID: MJb7UGxaRcqWExCwsQzOgA
  2024-07-03 17:08:27 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-1] ProducerId set to 70 with epoch 0
  2024-07-03 17:08:27 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-07-03 17:08:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-07-03 17:08:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-07-03 17:08:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-07-03 17:08:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-1 unregistered
  2024-07-03 17:08:27 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-07-03 17:08:27 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019308028
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-1, groupId=test] Subscribed to topic(s): yang.tests
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-1, groupId=test] Cluster ID: MJb7UGxaRcqWExCwsQzOgA
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-1, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-1-a89694f2-58b7-4ba1-83ae-3fa49a3835f4
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-1, groupId=test] Successfully joined group with generation Generation{generationId=1, memberId='consumer-test-1-a89694f2-58b7-4ba1-83ae-3fa49a3835f4', protocol='range'}
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-1, groupId=test] Finished assignment for group at generation 1: {consumer-test-1-a89694f2-58b7-4ba1-83ae-3fa49a3835f4=Assignment(partitions=[yang.tests-0])}
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-1, groupId=test] Successfully synced group in generation Generation{generationId=1, memberId='consumer-test-1-a89694f2-58b7-4ba1-83ae-3fa49a3835f4', protocol='range'}
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-1, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-1, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(1578) -[Consumer clientId=consumer-test-1, groupId=test] Found no committed offset for partition yang.tests-0
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.clients.consumer.internals.SubscriptionState -SubscriptionState.java(398) -[Consumer clientId=consumer-test-1, groupId=test] Resetting offset for partition yang.tests-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}.
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-1, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-1, groupId=test] Member consumer-test-1-a89694f2-58b7-4ba1-83ae-3fa49a3835f4 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-1, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-1 unregistered
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-07-03 17:08:28 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-2] Instantiated an idempotent producer.
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019308629
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-2] Cluster ID: MJb7UGxaRcqWExCwsQzOgA
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-2] ProducerId set to 71 with epoch 0
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-07-03 17:08:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-2 unregistered
  2024-07-03 17:11:08 [ INFO] - org.apache.kafka.clients.admin.AdminClientConfig -AbstractConfig.java(370) -AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = java-admin-client
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

  2024-07-03 17:11:08 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:11:08 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:11:08 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019468706
  2024-07-03 17:11:09 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-07-03 17:11:09 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-07-03 17:11:09 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-1] Instantiated an idempotent producer.
  2024-07-03 17:11:09 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:11:09 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:11:09 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019469908
  2024-07-03 17:11:09 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {yang.tests=LEADER_NOT_AVAILABLE}
  2024-07-03 17:11:09 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-1] Cluster ID: MJb7UGxaRcqWExCwsQzOgA
  2024-07-03 17:11:09 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-1] ProducerId set to 72 with epoch 0
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-1 unregistered
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-07-03 17:11:10 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019470249
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-1, groupId=test] Subscribed to topic(s): yang.tests
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-1, groupId=test] Cluster ID: MJb7UGxaRcqWExCwsQzOgA
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-1, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-1-9175a53b-bd49-493d-a078-5814a9e91763
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-1, groupId=test] Successfully joined group with generation Generation{generationId=1, memberId='consumer-test-1-9175a53b-bd49-493d-a078-5814a9e91763', protocol='range'}
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-1, groupId=test] Finished assignment for group at generation 1: {consumer-test-1-9175a53b-bd49-493d-a078-5814a9e91763=Assignment(partitions=[yang.tests-0])}
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-1, groupId=test] Successfully synced group in generation Generation{generationId=1, memberId='consumer-test-1-9175a53b-bd49-493d-a078-5814a9e91763', protocol='range'}
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-1, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-1, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(1578) -[Consumer clientId=consumer-test-1, groupId=test] Found no committed offset for partition yang.tests-0
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.clients.consumer.internals.SubscriptionState -SubscriptionState.java(398) -[Consumer clientId=consumer-test-1, groupId=test] Resetting offset for partition yang.tests-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}.
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-1, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-1, groupId=test] Member consumer-test-1-9175a53b-bd49-493d-a078-5814a9e91763 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-1, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-1 unregistered
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-07-03 17:11:10 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-2] Instantiated an idempotent producer.
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019470881
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-2] Cluster ID: MJb7UGxaRcqWExCwsQzOgA
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-2] ProducerId set to 73 with epoch 0
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-07-03 17:11:10 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-2 unregistered
  2024-07-03 17:11:54 [ INFO] - org.apache.kafka.clients.admin.AdminClientConfig -AbstractConfig.java(370) -AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = java-admin-client
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

  2024-07-03 17:11:55 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:11:55 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:11:55 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019515115
  2024-07-03 17:11:55 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-07-03 17:11:55 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-1] Instantiated an idempotent producer.
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019516102
  2024-07-03 17:11:56 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {yang.tests=LEADER_NOT_AVAILABLE}
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-1] Cluster ID: MJb7UGxaRcqWExCwsQzOgA
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-1] ProducerId set to 74 with epoch 0
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-1 unregistered
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-07-03 17:11:56 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019516436
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-1, groupId=test] Subscribed to topic(s): yang.tests
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-1, groupId=test] Cluster ID: MJb7UGxaRcqWExCwsQzOgA
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-1, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-1-d56ffa9c-010a-485b-8077-e8e2a4ff6715
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-1, groupId=test] Successfully joined group with generation Generation{generationId=1, memberId='consumer-test-1-d56ffa9c-010a-485b-8077-e8e2a4ff6715', protocol='range'}
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-1, groupId=test] Finished assignment for group at generation 1: {consumer-test-1-d56ffa9c-010a-485b-8077-e8e2a4ff6715=Assignment(partitions=[yang.tests-0])}
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-1, groupId=test] Successfully synced group in generation Generation{generationId=1, memberId='consumer-test-1-d56ffa9c-010a-485b-8077-e8e2a4ff6715', protocol='range'}
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-1, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-1, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(1578) -[Consumer clientId=consumer-test-1, groupId=test] Found no committed offset for partition yang.tests-0
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.clients.consumer.internals.SubscriptionState -SubscriptionState.java(398) -[Consumer clientId=consumer-test-1, groupId=test] Resetting offset for partition yang.tests-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}.
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-1, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-1, groupId=test] Member consumer-test-1-d56ffa9c-010a-485b-8077-e8e2a4ff6715 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-1, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-07-03 17:11:56 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-07-03 17:11:57 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-07-03 17:11:57 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-07-03 17:11:57 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-07-03 17:11:57 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-1 unregistered
  2024-07-03 17:11:57 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-07-03 17:11:57 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-07-03 17:11:57 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-2] Instantiated an idempotent producer.
  2024-07-03 17:11:57 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:11:57 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:11:57 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019517064
  2024-07-03 17:11:57 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-2] Cluster ID: MJb7UGxaRcqWExCwsQzOgA
  2024-07-03 17:11:57 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-2] ProducerId set to 75 with epoch 0
  2024-07-03 17:11:57 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-07-03 17:11:57 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-07-03 17:11:57 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-07-03 17:11:57 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-07-03 17:11:57 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-2 unregistered
  2024-07-03 17:12:10 [ INFO] - org.apache.kafka.clients.admin.AdminClientConfig -AbstractConfig.java(370) -AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = java-admin-client
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

  2024-07-03 17:12:10 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:12:10 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:12:10 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019530773
  2024-07-03 17:12:11 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-07-03 17:12:11 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-07-03 17:12:11 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-1] Instantiated an idempotent producer.
  2024-07-03 17:12:11 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:12:11 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:12:11 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019531779
  2024-07-03 17:12:11 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {yang.tests=LEADER_NOT_AVAILABLE}
  2024-07-03 17:12:11 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-1] Cluster ID: MJb7UGxaRcqWExCwsQzOgA
  2024-07-03 17:12:11 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-1] ProducerId set to 76 with epoch 0
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-1 unregistered
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-07-03 17:12:12 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019532207
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-1, groupId=test] Subscribed to topic(s): yang.tests
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-1, groupId=test] Cluster ID: MJb7UGxaRcqWExCwsQzOgA
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-1, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-1-43ea0b01-c8aa-4f05-98af-bdbdcfa330b3
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-1, groupId=test] Successfully joined group with generation Generation{generationId=1, memberId='consumer-test-1-43ea0b01-c8aa-4f05-98af-bdbdcfa330b3', protocol='range'}
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-1, groupId=test] Finished assignment for group at generation 1: {consumer-test-1-43ea0b01-c8aa-4f05-98af-bdbdcfa330b3=Assignment(partitions=[yang.tests-0])}
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-1, groupId=test] Successfully synced group in generation Generation{generationId=1, memberId='consumer-test-1-43ea0b01-c8aa-4f05-98af-bdbdcfa330b3', protocol='range'}
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-1, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-1, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(1578) -[Consumer clientId=consumer-test-1, groupId=test] Found no committed offset for partition yang.tests-0
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.clients.consumer.internals.SubscriptionState -SubscriptionState.java(398) -[Consumer clientId=consumer-test-1, groupId=test] Resetting offset for partition yang.tests-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}.
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-1, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-1, groupId=test] Member consumer-test-1-43ea0b01-c8aa-4f05-98af-bdbdcfa330b3 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-1, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-1 unregistered
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-07-03 17:12:12 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-2] Instantiated an idempotent producer.
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019532898
  2024-07-03 17:12:12 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-2] Error while fetching metadata with correlation id 1 : {yang.tests.error=LEADER_NOT_AVAILABLE}
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-2] Cluster ID: MJb7UGxaRcqWExCwsQzOgA
  2024-07-03 17:12:12 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-2] ProducerId set to 77 with epoch 0
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-2 unregistered
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-3
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-07-03 17:12:13 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-3] Instantiated an idempotent producer.
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019533141
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-3] Cluster ID: MJb7UGxaRcqWExCwsQzOgA
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-3] ProducerId set to 78 with epoch 0
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-3] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-3 unregistered
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-07-03 17:12:13 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019533187
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-2, groupId=test] Subscribed to topic(s): yang.tests
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-2, groupId=test] Cluster ID: MJb7UGxaRcqWExCwsQzOgA
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-2, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-2, groupId=test] (Re-)joining group
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-2, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-2-2158912c-487b-4817-bee2-f6a860797212
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-2, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-2, groupId=test] (Re-)joining group
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-2, groupId=test] Successfully joined group with generation Generation{generationId=3, memberId='consumer-test-2-2158912c-487b-4817-bee2-f6a860797212', protocol='range'}
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-2, groupId=test] Finished assignment for group at generation 3: {consumer-test-2-2158912c-487b-4817-bee2-f6a860797212=Assignment(partitions=[yang.tests-0])}
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-2, groupId=test] Successfully synced group in generation Generation{generationId=3, memberId='consumer-test-2-2158912c-487b-4817-bee2-f6a860797212', protocol='range'}
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-2, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-2, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-2, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-2, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-2, groupId=test] Member consumer-test-2-2158912c-487b-4817-bee2-f6a860797212 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-2, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-2, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-2 unregistered
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-4
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-07-03 17:12:13 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-4] Instantiated an idempotent producer.
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019533794
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-4] Cluster ID: MJb7UGxaRcqWExCwsQzOgA
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-4] ProducerId set to 79 with epoch 0
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-4] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-4 unregistered
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-07-03 17:12:13 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019533863
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-3, groupId=test] Subscribed to topic(s): yang.tests
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-3, groupId=test] Cluster ID: MJb7UGxaRcqWExCwsQzOgA
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-3, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-3, groupId=test] (Re-)joining group
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-3, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-3-ffd4dccb-30b9-495b-ad1a-f38131fc9e92
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-3, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-3, groupId=test] (Re-)joining group
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-3, groupId=test] Successfully joined group with generation Generation{generationId=5, memberId='consumer-test-3-ffd4dccb-30b9-495b-ad1a-f38131fc9e92', protocol='range'}
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-3, groupId=test] Finished assignment for group at generation 5: {consumer-test-3-ffd4dccb-30b9-495b-ad1a-f38131fc9e92=Assignment(partitions=[yang.tests-0])}
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-3, groupId=test] Successfully synced group in generation Generation{generationId=5, memberId='consumer-test-3-ffd4dccb-30b9-495b-ad1a-f38131fc9e92', protocol='range'}
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-3, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-3, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-3, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-3, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-3, groupId=test] Member consumer-test-3-ffd4dccb-30b9-495b-ad1a-f38131fc9e92 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-3, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-07-03 17:12:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-3, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-07-03 17:12:14 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-07-03 17:12:14 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-07-03 17:12:14 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-07-03 17:12:14 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-3 unregistered
  2024-07-03 17:12:14 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-5
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-07-03 17:12:14 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-07-03 17:12:14 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-5] Instantiated an idempotent producer.
  2024-07-03 17:12:14 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:12:14 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:12:14 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019534433
  2024-07-03 17:12:14 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-5] Cluster ID: MJb7UGxaRcqWExCwsQzOgA
  2024-07-03 17:12:14 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-5] ProducerId set to 80 with epoch 0
  2024-07-03 17:12:14 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-5] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-07-03 17:12:14 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-07-03 17:12:14 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-07-03 17:12:14 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-07-03 17:12:14 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-5 unregistered
  2024-07-03 17:18:24 [ INFO] - org.apache.kafka.clients.admin.AdminClientConfig -AbstractConfig.java(370) -AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = java-admin-client
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

  2024-07-03 17:18:25 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:18:25 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:18:25 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019905123
  2024-07-03 17:18:25 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-07-03 17:18:25 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-1] Instantiated an idempotent producer.
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019906169
  2024-07-03 17:18:26 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {yang.tests=LEADER_NOT_AVAILABLE}
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-1] Cluster ID: MJb7UGxaRcqWExCwsQzOgA
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-1] ProducerId set to 81 with epoch 0
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-1 unregistered
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-07-03 17:18:26 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019906509
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-1, groupId=test] Subscribed to topic(s): yang.tests
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-1, groupId=test] Cluster ID: MJb7UGxaRcqWExCwsQzOgA
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-1, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-1-5ed511fa-48b0-4b51-955b-ce14b0b59f14
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-1, groupId=test] Successfully joined group with generation Generation{generationId=1, memberId='consumer-test-1-5ed511fa-48b0-4b51-955b-ce14b0b59f14', protocol='range'}
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-1, groupId=test] Finished assignment for group at generation 1: {consumer-test-1-5ed511fa-48b0-4b51-955b-ce14b0b59f14=Assignment(partitions=[yang.tests-0])}
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-1, groupId=test] Successfully synced group in generation Generation{generationId=1, memberId='consumer-test-1-5ed511fa-48b0-4b51-955b-ce14b0b59f14', protocol='range'}
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-1, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-1, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(1578) -[Consumer clientId=consumer-test-1, groupId=test] Found no committed offset for partition yang.tests-0
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.clients.consumer.internals.SubscriptionState -SubscriptionState.java(398) -[Consumer clientId=consumer-test-1, groupId=test] Resetting offset for partition yang.tests-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}.
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-1, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-1, groupId=test] Member consumer-test-1-5ed511fa-48b0-4b51-955b-ce14b0b59f14 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-1, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-07-03 17:18:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-1 unregistered
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-07-03 17:18:27 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-2] Instantiated an idempotent producer.
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019907173
  2024-07-03 17:18:27 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-2] Error while fetching metadata with correlation id 1 : {yang.tests.error=LEADER_NOT_AVAILABLE}
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-2] Cluster ID: MJb7UGxaRcqWExCwsQzOgA
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-2] ProducerId set to 82 with epoch 0
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-2 unregistered
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-3
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-07-03 17:18:27 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = false
	yang.json.fail.unknown.properties = true

  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-3] Instantiated an idempotent producer.
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019907431
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-3] Cluster ID: MJb7UGxaRcqWExCwsQzOgA
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-3] ProducerId set to 83 with epoch 0
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-3] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-3 unregistered
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-07-03 17:18:27 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019907475
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-2, groupId=test] Subscribed to topic(s): yang.tests
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-2, groupId=test] Cluster ID: MJb7UGxaRcqWExCwsQzOgA
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-2, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-2, groupId=test] (Re-)joining group
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-2, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-2-c3d62b3b-959d-448a-a799-4a968a6aed76
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-2, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-2, groupId=test] (Re-)joining group
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-2, groupId=test] Successfully joined group with generation Generation{generationId=3, memberId='consumer-test-2-c3d62b3b-959d-448a-a799-4a968a6aed76', protocol='range'}
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-2, groupId=test] Finished assignment for group at generation 3: {consumer-test-2-c3d62b3b-959d-448a-a799-4a968a6aed76=Assignment(partitions=[yang.tests-0])}
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-2, groupId=test] Successfully synced group in generation Generation{generationId=3, memberId='consumer-test-2-c3d62b3b-959d-448a-a799-4a968a6aed76', protocol='range'}
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-2, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-2, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-2, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(1564) -[Consumer clientId=consumer-test-2, groupId=test] Seeking to offset 2 for partition yang.tests-0
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-2, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-2, groupId=test] Member consumer-test-2-c3d62b3b-959d-448a-a799-4a968a6aed76 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-2, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-2, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-2 unregistered
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-4
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-07-03 17:18:27 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-4] Instantiated an idempotent producer.
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019907535
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-4] Cluster ID: MJb7UGxaRcqWExCwsQzOgA
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-4] ProducerId set to 84 with epoch 0
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-4] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-4 unregistered
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-07-03 17:18:27 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019907577
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-3, groupId=test] Subscribed to topic(s): yang.tests
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-3, groupId=test] Cluster ID: MJb7UGxaRcqWExCwsQzOgA
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-3, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-3, groupId=test] (Re-)joining group
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-3, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-3-a8707fc5-533b-45f2-b124-b912b5c41d00
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-3, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-3, groupId=test] (Re-)joining group
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-3, groupId=test] Successfully joined group with generation Generation{generationId=5, memberId='consumer-test-3-a8707fc5-533b-45f2-b124-b912b5c41d00', protocol='range'}
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-3, groupId=test] Finished assignment for group at generation 5: {consumer-test-3-a8707fc5-533b-45f2-b124-b912b5c41d00=Assignment(partitions=[yang.tests-0])}
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-3, groupId=test] Successfully synced group in generation Generation{generationId=5, memberId='consumer-test-3-a8707fc5-533b-45f2-b124-b912b5c41d00', protocol='range'}
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-3, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-3, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-3, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-3, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-3, groupId=test] Member consumer-test-3-a8707fc5-533b-45f2-b124-b912b5c41d00 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-3, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-07-03 17:18:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-3, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-07-03 17:18:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-07-03 17:18:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-07-03 17:18:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-07-03 17:18:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-3 unregistered
  2024-07-03 17:18:28 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-5
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-07-03 17:18:28 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-07-03 17:18:28 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-5] Instantiated an idempotent producer.
  2024-07-03 17:18:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-07-03 17:18:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-07-03 17:18:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1720019908162
  2024-07-03 17:18:28 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-5] Cluster ID: MJb7UGxaRcqWExCwsQzOgA
  2024-07-03 17:18:28 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-5] ProducerId set to 85 with epoch 0
  2024-07-03 17:18:28 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-5] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-07-03 17:18:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-07-03 17:18:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-07-03 17:18:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-07-03 17:18:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-5 unregistered
  