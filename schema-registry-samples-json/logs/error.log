2024-08-29 19:41:42 [ INFO] - org.apache.kafka.clients.admin.AdminClientConfig -AbstractConfig.java(370) -AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = java-admin-client
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

  2024-08-29 19:41:42 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:41:42 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:41:42 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724953302278
  2024-08-29 19:41:43 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:41:43 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 19:41:43 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-1] Instantiated an idempotent producer.
  2024-08-29 19:41:43 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:41:43 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:41:43 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724953303509
  2024-08-29 19:41:43 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {yang.tests=LEADER_NOT_AVAILABLE}
  2024-08-29 19:41:43 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-1] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:41:43 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-1] ProducerId set to 0 with epoch 0
  2024-08-29 19:41:43 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:41:43 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:41:43 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:41:43 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:41:43 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-1 unregistered
  2024-08-29 19:41:43 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:41:43 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 19:41:43 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-2] Instantiated an idempotent producer.
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724953304013
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-2] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-2] ProducerId set to 1 with epoch 0
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-2 unregistered
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-3
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:41:44 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-3] Instantiated an idempotent producer.
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724953304177
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-3] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-3] ProducerId set to 2 with epoch 0
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-3] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-3 unregistered
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-4
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:41:44 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-4] Instantiated an idempotent producer.
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724953304252
  2024-08-29 19:41:44 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-4] Error while fetching metadata with correlation id 1 : {yang.tests.error=LEADER_NOT_AVAILABLE}
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-4] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-4] ProducerId set to 3 with epoch 0
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-4] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-4 unregistered
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-5
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:41:44 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = false
	yang.json.fail.unknown.properties = true

  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-5] Instantiated an idempotent producer.
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724953304417
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-5] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-5] ProducerId set to 4 with epoch 0
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-5] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-5 unregistered
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 19:41:44 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724953304514
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-1, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-1, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-1, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-1-3a1c3ef2-3f60-4536-a2e9-69dc7ea518ac
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-1, groupId=test] Successfully joined group with generation Generation{generationId=1, memberId='consumer-test-1-3a1c3ef2-3f60-4536-a2e9-69dc7ea518ac', protocol='range'}
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-1, groupId=test] Finished assignment for group at generation 1: {consumer-test-1-3a1c3ef2-3f60-4536-a2e9-69dc7ea518ac=Assignment(partitions=[yang.tests-0])}
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-1, groupId=test] Successfully synced group in generation Generation{generationId=1, memberId='consumer-test-1-3a1c3ef2-3f60-4536-a2e9-69dc7ea518ac', protocol='range'}
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-1, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-1, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(1578) -[Consumer clientId=consumer-test-1, groupId=test] Found no committed offset for partition yang.tests-0
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.consumer.internals.SubscriptionState -SubscriptionState.java(398) -[Consumer clientId=consumer-test-1, groupId=test] Resetting offset for partition yang.tests-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}.
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(1564) -[Consumer clientId=consumer-test-1, groupId=test] Seeking to offset 1 for partition yang.tests-0
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-1, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-1, groupId=test] Member consumer-test-1-3a1c3ef2-3f60-4536-a2e9-69dc7ea518ac sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-1, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-1 unregistered
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-6
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:41:44 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-6] Instantiated an idempotent producer.
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724953304692
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-6] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-6] ProducerId set to 5 with epoch 0
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-6] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-6 unregistered
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-7
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:41:44 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-7] Instantiated an idempotent producer.
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724953304747
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-7] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-7] ProducerId set to 6 with epoch 0
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-7] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-7 unregistered
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-8
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:41:44 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-8] Instantiated an idempotent producer.
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724953304791
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-8] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-8] ProducerId set to 7 with epoch 0
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-8] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-8 unregistered
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-9
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:41:44 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-9] Instantiated an idempotent producer.
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724953304834
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-9] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-9] ProducerId set to 8 with epoch 0
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-9] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-9 unregistered
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-10
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:41:44 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-10] Instantiated an idempotent producer.
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724953304927
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-10] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-10] ProducerId set to 9 with epoch 0
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-10] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-10 unregistered
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-11
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:41:44 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = false
	yang.json.fail.unknown.properties = true

  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-11] Instantiated an idempotent producer.
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:41:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724953304993
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-11] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-11] ProducerId set to 10 with epoch 0
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-11] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-11 unregistered
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 19:41:45 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724953305050
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-2, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-2, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-2, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-2, groupId=test] (Re-)joining group
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-2, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-2-e4fd64e6-c5d0-453f-8eeb-51b043a9c326
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-2, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-2, groupId=test] (Re-)joining group
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-2, groupId=test] Successfully joined group with generation Generation{generationId=3, memberId='consumer-test-2-e4fd64e6-c5d0-453f-8eeb-51b043a9c326', protocol='range'}
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-2, groupId=test] Finished assignment for group at generation 3: {consumer-test-2-e4fd64e6-c5d0-453f-8eeb-51b043a9c326=Assignment(partitions=[yang.tests-0])}
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-2, groupId=test] Successfully synced group in generation Generation{generationId=3, memberId='consumer-test-2-e4fd64e6-c5d0-453f-8eeb-51b043a9c326', protocol='range'}
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-2, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-2, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-2, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(1564) -[Consumer clientId=consumer-test-2, groupId=test] Seeking to offset 2 for partition yang.tests-0
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-2, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-2, groupId=test] Member consumer-test-2-e4fd64e6-c5d0-453f-8eeb-51b043a9c326 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-2, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-2, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:41:45 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-2 unregistered
  2024-08-29 19:47:24 [ INFO] - org.apache.kafka.clients.admin.AdminClientConfig -AbstractConfig.java(370) -AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = java-admin-client
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

  2024-08-29 19:47:24 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:47:24 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:47:24 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724953644690
  2024-08-29 19:47:25 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:47:25 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 19:47:25 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-1] Instantiated an idempotent producer.
  2024-08-29 19:47:25 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:47:25 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:47:25 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724953645866
  2024-08-29 19:47:25 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {yang.tests=LEADER_NOT_AVAILABLE}
  2024-08-29 19:47:25 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-1] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:47:25 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-1] ProducerId set to 11 with epoch 0
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-1 unregistered
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:47:26 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-2] Instantiated an idempotent producer.
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724953646226
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-2] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-2] ProducerId set to 12 with epoch 0
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-2 unregistered
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-3
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:47:26 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-3] Instantiated an idempotent producer.
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724953646340
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-3] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-3] ProducerId set to 13 with epoch 0
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-3] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-3 unregistered
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-4
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:47:26 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-4] Instantiated an idempotent producer.
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724953646391
  2024-08-29 19:47:26 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-4] Error while fetching metadata with correlation id 1 : {yang.tests.error=LEADER_NOT_AVAILABLE}
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-4] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-4] ProducerId set to 14 with epoch 0
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-4] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-4 unregistered
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-5
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:47:26 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = false
	yang.json.fail.unknown.properties = true

  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-5] Instantiated an idempotent producer.
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724953646577
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-5] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-5] ProducerId set to 15 with epoch 0
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-5] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-5 unregistered
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 19:47:26 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724953646753
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-1, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-1, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-1, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-1-4f8ca1f8-4114-4fd4-a590-4a2a266f50f8
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-1, groupId=test] Successfully joined group with generation Generation{generationId=1, memberId='consumer-test-1-4f8ca1f8-4114-4fd4-a590-4a2a266f50f8', protocol='range'}
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-1, groupId=test] Finished assignment for group at generation 1: {consumer-test-1-4f8ca1f8-4114-4fd4-a590-4a2a266f50f8=Assignment(partitions=[yang.tests-0])}
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-1, groupId=test] Successfully synced group in generation Generation{generationId=1, memberId='consumer-test-1-4f8ca1f8-4114-4fd4-a590-4a2a266f50f8', protocol='range'}
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-1, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-1, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(1578) -[Consumer clientId=consumer-test-1, groupId=test] Found no committed offset for partition yang.tests-0
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.consumer.internals.SubscriptionState -SubscriptionState.java(398) -[Consumer clientId=consumer-test-1, groupId=test] Resetting offset for partition yang.tests-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}.
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(1564) -[Consumer clientId=consumer-test-1, groupId=test] Seeking to offset 1 for partition yang.tests-0
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-1, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-1, groupId=test] Member consumer-test-1-4f8ca1f8-4114-4fd4-a590-4a2a266f50f8 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-1, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-1 unregistered
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-6
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:47:26 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-6] Instantiated an idempotent producer.
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724953646868
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-6] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-6] ProducerId set to 16 with epoch 0
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-6] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-6 unregistered
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-7
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:47:26 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-7] Instantiated an idempotent producer.
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724953646913
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-7] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-7] ProducerId set to 17 with epoch 0
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-7] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-7 unregistered
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-8
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:47:26 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-8] Instantiated an idempotent producer.
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724953646952
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-8] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-8] ProducerId set to 18 with epoch 0
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-8] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-8 unregistered
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-9
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:47:26 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-9] Instantiated an idempotent producer.
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724953646991
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-9] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:47:26 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-9] ProducerId set to 19 with epoch 0
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-9] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-9 unregistered
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-10
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:47:27 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-10] Instantiated an idempotent producer.
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724953647073
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-10] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-10] ProducerId set to 20 with epoch 0
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-10] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-10 unregistered
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-11
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:47:27 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = false
	yang.json.fail.unknown.properties = true

  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-11] Instantiated an idempotent producer.
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724953647123
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-11] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-11] ProducerId set to 21 with epoch 0
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-11] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-11 unregistered
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 19:47:27 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724953647176
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-2, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-2, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-2, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-2, groupId=test] (Re-)joining group
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-2, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-2-5236301a-8017-4776-b71c-fce129a4de64
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-2, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-2, groupId=test] (Re-)joining group
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-2, groupId=test] Successfully joined group with generation Generation{generationId=3, memberId='consumer-test-2-5236301a-8017-4776-b71c-fce129a4de64', protocol='range'}
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-2, groupId=test] Finished assignment for group at generation 3: {consumer-test-2-5236301a-8017-4776-b71c-fce129a4de64=Assignment(partitions=[yang.tests-0])}
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-2, groupId=test] Successfully synced group in generation Generation{generationId=3, memberId='consumer-test-2-5236301a-8017-4776-b71c-fce129a4de64', protocol='range'}
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-2, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-2, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-2, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(1564) -[Consumer clientId=consumer-test-2, groupId=test] Seeking to offset 2 for partition yang.tests-0
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-2, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-2, groupId=test] Member consumer-test-2-5236301a-8017-4776-b71c-fce129a4de64 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-2, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-2, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:47:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-2 unregistered
  2024-08-29 19:55:45 [ INFO] - org.apache.kafka.clients.admin.AdminClientConfig -AbstractConfig.java(370) -AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = java-admin-client
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

  2024-08-29 19:55:45 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:55:45 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:55:45 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724954145327
  2024-08-29 19:55:46 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:55:46 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 19:55:46 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-1] Instantiated an idempotent producer.
  2024-08-29 19:55:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:55:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:55:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724954146733
  2024-08-29 19:55:46 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {yang.tests=LEADER_NOT_AVAILABLE}
  2024-08-29 19:55:46 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-1] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:55:46 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-1] ProducerId set to 22 with epoch 0
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-1 unregistered
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:55:47 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-2] Instantiated an idempotent producer.
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724954147147
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-2] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-2] ProducerId set to 23 with epoch 0
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-2 unregistered
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-3
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:55:47 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-3] Instantiated an idempotent producer.
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724954147327
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-3] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-3] ProducerId set to 24 with epoch 0
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-3] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-3 unregistered
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-4
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:55:47 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-4] Instantiated an idempotent producer.
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724954147423
  2024-08-29 19:55:47 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-4] Error while fetching metadata with correlation id 1 : {yang.tests.error=LEADER_NOT_AVAILABLE}
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-4] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-4] ProducerId set to 25 with epoch 0
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-4] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-4 unregistered
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-5
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:55:47 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = false
	yang.json.fail.unknown.properties = true

  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-5] Instantiated an idempotent producer.
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724954147575
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-5] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-5] ProducerId set to 26 with epoch 0
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-5] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-5 unregistered
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 19:55:47 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724954147653
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-1, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-1, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-1, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-1-4bffc6c1-4b55-4c56-b2ba-02ed82796bb4
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-1, groupId=test] Successfully joined group with generation Generation{generationId=1, memberId='consumer-test-1-4bffc6c1-4b55-4c56-b2ba-02ed82796bb4', protocol='range'}
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-1, groupId=test] Finished assignment for group at generation 1: {consumer-test-1-4bffc6c1-4b55-4c56-b2ba-02ed82796bb4=Assignment(partitions=[yang.tests-0])}
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-1, groupId=test] Successfully synced group in generation Generation{generationId=1, memberId='consumer-test-1-4bffc6c1-4b55-4c56-b2ba-02ed82796bb4', protocol='range'}
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-1, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-1, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(1578) -[Consumer clientId=consumer-test-1, groupId=test] Found no committed offset for partition yang.tests-0
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.consumer.internals.SubscriptionState -SubscriptionState.java(398) -[Consumer clientId=consumer-test-1, groupId=test] Resetting offset for partition yang.tests-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}.
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(1564) -[Consumer clientId=consumer-test-1, groupId=test] Seeking to offset 1 for partition yang.tests-0
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-1, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-1, groupId=test] Member consumer-test-1-4bffc6c1-4b55-4c56-b2ba-02ed82796bb4 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-1, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-1 unregistered
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-6
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:55:47 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-6] Instantiated an idempotent producer.
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724954147761
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-6] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-6] ProducerId set to 27 with epoch 0
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-6] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-6 unregistered
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-7
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:55:47 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-7] Instantiated an idempotent producer.
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724954147808
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-7] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-7] ProducerId set to 28 with epoch 0
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-7] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-7 unregistered
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-8
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:55:47 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-8] Instantiated an idempotent producer.
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724954147851
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-8] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-8] ProducerId set to 29 with epoch 0
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-8] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-8 unregistered
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-9
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:55:47 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-9] Instantiated an idempotent producer.
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724954147895
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-9] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-9] ProducerId set to 30 with epoch 0
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-9] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-9 unregistered
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-10
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:55:47 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-10] Instantiated an idempotent producer.
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724954147973
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-10] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:55:47 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-10] ProducerId set to 31 with epoch 0
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-10] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-10 unregistered
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-11
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 19:55:48 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = false
	yang.json.fail.unknown.properties = true

  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-11] Instantiated an idempotent producer.
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724954148041
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-11] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-11] ProducerId set to 32 with epoch 0
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-11] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-11 unregistered
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 19:55:48 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724954148080
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-2, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-2, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-2, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-2, groupId=test] (Re-)joining group
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-2, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-2-64c133b2-8b0d-458e-bc71-f767eb503595
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-2, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-2, groupId=test] (Re-)joining group
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-2, groupId=test] Successfully joined group with generation Generation{generationId=3, memberId='consumer-test-2-64c133b2-8b0d-458e-bc71-f767eb503595', protocol='range'}
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-2, groupId=test] Finished assignment for group at generation 3: {consumer-test-2-64c133b2-8b0d-458e-bc71-f767eb503595=Assignment(partitions=[yang.tests-0])}
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-2, groupId=test] Successfully synced group in generation Generation{generationId=3, memberId='consumer-test-2-64c133b2-8b0d-458e-bc71-f767eb503595', protocol='range'}
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-2, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-2, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-2, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(1564) -[Consumer clientId=consumer-test-2, groupId=test] Seeking to offset 2 for partition yang.tests-0
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-2, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-2, groupId=test] Member consumer-test-2-64c133b2-8b0d-458e-bc71-f767eb503595 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-2, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-2, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 19:55:48 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-2 unregistered
  2024-08-29 20:02:51 [ INFO] - org.apache.kafka.clients.admin.AdminClientConfig -AbstractConfig.java(370) -AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = java-admin-client
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

  2024-08-29 20:02:51 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:02:51 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:02:51 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724954571291
  2024-08-29 20:02:52 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:02:52 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:02:52 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-1] Instantiated an idempotent producer.
  2024-08-29 20:02:52 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:02:52 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:02:52 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724954572892
  2024-08-29 20:02:52 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {yang.tests=LEADER_NOT_AVAILABLE}
  2024-08-29 20:02:52 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-1] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:02:52 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-1] ProducerId set to 33 with epoch 0
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-1 unregistered
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:02:53 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724954573321
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-1, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-1, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-1, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-1-f6f2f3a9-b8e3-49a7-b850-55a9f350c6f6
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-1, groupId=test] Successfully joined group with generation Generation{generationId=1, memberId='consumer-test-1-f6f2f3a9-b8e3-49a7-b850-55a9f350c6f6', protocol='range'}
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-1, groupId=test] Finished assignment for group at generation 1: {consumer-test-1-f6f2f3a9-b8e3-49a7-b850-55a9f350c6f6=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-1, groupId=test] Successfully synced group in generation Generation{generationId=1, memberId='consumer-test-1-f6f2f3a9-b8e3-49a7-b850-55a9f350c6f6', protocol='range'}
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-1, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-1, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(1578) -[Consumer clientId=consumer-test-1, groupId=test] Found no committed offset for partition yang.tests-0
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.clients.consumer.internals.SubscriptionState -SubscriptionState.java(398) -[Consumer clientId=consumer-test-1, groupId=test] Resetting offset for partition yang.tests-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}.
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-1, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-1, groupId=test] Member consumer-test-1-f6f2f3a9-b8e3-49a7-b850-55a9f350c6f6 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-1, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:02:53 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-1 unregistered
  2024-08-29 20:14:15 [ INFO] - org.apache.kafka.clients.admin.AdminClientConfig -AbstractConfig.java(370) -AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = java-admin-client
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

  2024-08-29 20:14:15 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:14:15 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:14:15 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724955255221
  2024-08-29 20:14:16 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:14:16 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:14:16 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-1] Instantiated an idempotent producer.
  2024-08-29 20:14:16 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:14:16 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:14:16 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724955256509
  2024-08-29 20:14:16 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {yang.tests=LEADER_NOT_AVAILABLE}
  2024-08-29 20:14:16 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-1] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:14:16 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-1] ProducerId set to 34 with epoch 0
  2024-08-29 20:14:16 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:14:16 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:14:16 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:14:16 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:14:16 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-1 unregistered
  2024-08-29 20:20:44 [ INFO] - org.apache.kafka.clients.admin.AdminClientConfig -AbstractConfig.java(370) -AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = java-admin-client
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

  2024-08-29 20:20:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:20:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:20:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724955644591
  2024-08-29 20:20:45 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:20:45 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:20:45 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-1] Instantiated an idempotent producer.
  2024-08-29 20:20:45 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:20:45 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:20:45 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724955645742
  2024-08-29 20:20:45 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {yang.tests=LEADER_NOT_AVAILABLE}
  2024-08-29 20:20:45 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-1] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:20:45 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-1] ProducerId set to 35 with epoch 0
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-1 unregistered
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:20:46 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724955646151
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-1, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-1, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-1, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-1-afcbc814-2472-49f2-bafa-8b9a3803eba2
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-1, groupId=test] Successfully joined group with generation Generation{generationId=1, memberId='consumer-test-1-afcbc814-2472-49f2-bafa-8b9a3803eba2', protocol='range'}
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-1, groupId=test] Finished assignment for group at generation 1: {consumer-test-1-afcbc814-2472-49f2-bafa-8b9a3803eba2=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-1, groupId=test] Successfully synced group in generation Generation{generationId=1, memberId='consumer-test-1-afcbc814-2472-49f2-bafa-8b9a3803eba2', protocol='range'}
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-1, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-1, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(1578) -[Consumer clientId=consumer-test-1, groupId=test] Found no committed offset for partition yang.tests-0
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.clients.consumer.internals.SubscriptionState -SubscriptionState.java(398) -[Consumer clientId=consumer-test-1, groupId=test] Resetting offset for partition yang.tests-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}.
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-1, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-1, groupId=test] Member consumer-test-1-afcbc814-2472-49f2-bafa-8b9a3803eba2 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-1, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:20:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-1 unregistered
  2024-08-29 20:21:26 [ INFO] - org.apache.kafka.clients.admin.AdminClientConfig -AbstractConfig.java(370) -AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = java-admin-client
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

  2024-08-29 20:21:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:21:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:21:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724955686239
  2024-08-29 20:21:26 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:21:26 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-1] Instantiated an idempotent producer.
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724955687149
  2024-08-29 20:21:27 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {yang.tests=LEADER_NOT_AVAILABLE}
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-1] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-1] ProducerId set to 36 with epoch 0
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-1 unregistered
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:21:27 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724955687451
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-1, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-1, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-1, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-1-d3de0640-e8e6-4958-8460-2321ba25783c
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-1, groupId=test] Successfully joined group with generation Generation{generationId=1, memberId='consumer-test-1-d3de0640-e8e6-4958-8460-2321ba25783c', protocol='range'}
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-1, groupId=test] Finished assignment for group at generation 1: {consumer-test-1-d3de0640-e8e6-4958-8460-2321ba25783c=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-1, groupId=test] Successfully synced group in generation Generation{generationId=1, memberId='consumer-test-1-d3de0640-e8e6-4958-8460-2321ba25783c', protocol='range'}
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-1, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-1, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(1578) -[Consumer clientId=consumer-test-1, groupId=test] Found no committed offset for partition yang.tests-0
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.clients.consumer.internals.SubscriptionState -SubscriptionState.java(398) -[Consumer clientId=consumer-test-1, groupId=test] Resetting offset for partition yang.tests-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}.
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-1, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-1, groupId=test] Member consumer-test-1-d3de0640-e8e6-4958-8460-2321ba25783c sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-1, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:21:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:21:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:21:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:21:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:21:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-1 unregistered
  2024-08-29 20:31:43 [ INFO] - org.apache.kafka.clients.admin.AdminClientConfig -AbstractConfig.java(370) -AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = java-admin-client
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

  2024-08-29 20:31:43 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:43 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:43 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956303431
  2024-08-29 20:31:44 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:31:44 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:31:44 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-1] Instantiated an idempotent producer.
  2024-08-29 20:31:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:44 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956304526
  2024-08-29 20:31:44 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {yang.tests=LEADER_NOT_AVAILABLE}
  2024-08-29 20:31:44 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-1] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:44 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-1] ProducerId set to 37 with epoch 0
  2024-08-29 20:31:44 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-1 unregistered
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:31:45 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956305065
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-1, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-1, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-1, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-1-b19ea237-bc92-4972-a701-bdcefffd4cd6
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-1, groupId=test] Successfully joined group with generation Generation{generationId=1, memberId='consumer-test-1-b19ea237-bc92-4972-a701-bdcefffd4cd6', protocol='range'}
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-1, groupId=test] Finished assignment for group at generation 1: {consumer-test-1-b19ea237-bc92-4972-a701-bdcefffd4cd6=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-1, groupId=test] Successfully synced group in generation Generation{generationId=1, memberId='consumer-test-1-b19ea237-bc92-4972-a701-bdcefffd4cd6', protocol='range'}
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-1, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-1, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(1578) -[Consumer clientId=consumer-test-1, groupId=test] Found no committed offset for partition yang.tests-0
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.SubscriptionState -SubscriptionState.java(398) -[Consumer clientId=consumer-test-1, groupId=test] Resetting offset for partition yang.tests-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}.
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-1, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-1, groupId=test] Member consumer-test-1-b19ea237-bc92-4972-a701-bdcefffd4cd6 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-1, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-1 unregistered
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:31:45 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-2] Instantiated an idempotent producer.
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956305714
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-2] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-2] ProducerId set to 38 with epoch 0
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-2 unregistered
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:31:45 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956305813
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-2, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-2, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-2, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-2, groupId=test] (Re-)joining group
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-2, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-2-5cef6845-34b0-4075-b653-8d2d9c966eb1
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-2, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-2, groupId=test] (Re-)joining group
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-2, groupId=test] Successfully joined group with generation Generation{generationId=3, memberId='consumer-test-2-5cef6845-34b0-4075-b653-8d2d9c966eb1', protocol='range'}
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-2, groupId=test] Finished assignment for group at generation 3: {consumer-test-2-5cef6845-34b0-4075-b653-8d2d9c966eb1=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-2, groupId=test] Successfully synced group in generation Generation{generationId=3, memberId='consumer-test-2-5cef6845-34b0-4075-b653-8d2d9c966eb1', protocol='range'}
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-2, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-2, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-2, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-2, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-2, groupId=test] Member consumer-test-2-5cef6845-34b0-4075-b653-8d2d9c966eb1 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-2, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:31:45 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-2, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-2 unregistered
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-3
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:31:46 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-3] Instantiated an idempotent producer.
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956306411
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-3] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-3] ProducerId set to 39 with epoch 0
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-3] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-3 unregistered
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:31:46 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956306463
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-3, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-3, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-3, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-3, groupId=test] (Re-)joining group
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-3, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-3-7e700b87-1e37-435e-ade1-aa57973c33f5
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-3, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-3, groupId=test] (Re-)joining group
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-3, groupId=test] Successfully joined group with generation Generation{generationId=5, memberId='consumer-test-3-7e700b87-1e37-435e-ade1-aa57973c33f5', protocol='range'}
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-3, groupId=test] Finished assignment for group at generation 5: {consumer-test-3-7e700b87-1e37-435e-ade1-aa57973c33f5=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-3, groupId=test] Successfully synced group in generation Generation{generationId=5, memberId='consumer-test-3-7e700b87-1e37-435e-ade1-aa57973c33f5', protocol='range'}
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-3, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-3, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-3, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-3, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-3, groupId=test] Member consumer-test-3-7e700b87-1e37-435e-ade1-aa57973c33f5 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-3, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-3, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-3 unregistered
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-4
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:31:46 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-4] Instantiated an idempotent producer.
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956306607
  2024-08-29 20:31:46 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-4] Error while fetching metadata with correlation id 1 : {yang.tests.error=LEADER_NOT_AVAILABLE}
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-4] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-4] ProducerId set to 40 with epoch 0
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-4] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-4 unregistered
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-5
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:31:46 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = false
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-5] Instantiated an idempotent producer.
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956306774
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-5] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-5] ProducerId set to 41 with epoch 0
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-5] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-5 unregistered
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:31:46 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956306819
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-4, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-4, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-4, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-4, groupId=test] (Re-)joining group
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-4, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-4-7eaa64e5-e34e-4345-ae8b-75a4e75691d4
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-4, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-4, groupId=test] (Re-)joining group
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-4, groupId=test] Successfully joined group with generation Generation{generationId=7, memberId='consumer-test-4-7eaa64e5-e34e-4345-ae8b-75a4e75691d4', protocol='range'}
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-4, groupId=test] Finished assignment for group at generation 7: {consumer-test-4-7eaa64e5-e34e-4345-ae8b-75a4e75691d4=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-4, groupId=test] Successfully synced group in generation Generation{generationId=7, memberId='consumer-test-4-7eaa64e5-e34e-4345-ae8b-75a4e75691d4', protocol='range'}
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-4, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-4, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-4, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-4, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-4, groupId=test] Member consumer-test-4-7eaa64e5-e34e-4345-ae8b-75a4e75691d4 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-4, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-4, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-4 unregistered
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-6
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:31:46 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-6] Instantiated an idempotent producer.
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956306889
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-6] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-6] ProducerId set to 42 with epoch 0
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-6] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-6 unregistered
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:31:46 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956306941
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-5, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-5, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-5, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-5, groupId=test] (Re-)joining group
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-5, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-5-7d01ca5a-c024-4e00-8ae5-c1d7a37f89a4
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-5, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-5, groupId=test] (Re-)joining group
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-5, groupId=test] Successfully joined group with generation Generation{generationId=9, memberId='consumer-test-5-7d01ca5a-c024-4e00-8ae5-c1d7a37f89a4', protocol='range'}
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-5, groupId=test] Finished assignment for group at generation 9: {consumer-test-5-7d01ca5a-c024-4e00-8ae5-c1d7a37f89a4=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-5, groupId=test] Successfully synced group in generation Generation{generationId=9, memberId='consumer-test-5-7d01ca5a-c024-4e00-8ae5-c1d7a37f89a4', protocol='range'}
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-5, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-5, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-5, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(1564) -[Consumer clientId=consumer-test-5, groupId=test] Seeking to offset 4 for partition yang.tests-0
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-5, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-5, groupId=test] Member consumer-test-5-7d01ca5a-c024-4e00-8ae5-c1d7a37f89a4 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-5, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-5, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-5 unregistered
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-7
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:31:46 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-7] Instantiated an idempotent producer.
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:46 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956306998
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-7] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-7] ProducerId set to 43 with epoch 0
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-7] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-7 unregistered
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:31:47 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956307022
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-6, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-6, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-6, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-6, groupId=test] (Re-)joining group
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-6, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-6-80775555-c04e-4366-877f-751cfb787a58
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-6, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-6, groupId=test] (Re-)joining group
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-6, groupId=test] Successfully joined group with generation Generation{generationId=11, memberId='consumer-test-6-80775555-c04e-4366-877f-751cfb787a58', protocol='range'}
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-6, groupId=test] Finished assignment for group at generation 11: {consumer-test-6-80775555-c04e-4366-877f-751cfb787a58=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-6, groupId=test] Successfully synced group in generation Generation{generationId=11, memberId='consumer-test-6-80775555-c04e-4366-877f-751cfb787a58', protocol='range'}
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-6, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-6, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-6, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=4, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-6, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-6, groupId=test] Member consumer-test-6-80775555-c04e-4366-877f-751cfb787a58 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-6, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-6, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-6 unregistered
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-8
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:31:47 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-8] Instantiated an idempotent producer.
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956307595
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-8] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-8] ProducerId set to 44 with epoch 0
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-8] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-8 unregistered
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-9
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:31:47 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-9] Instantiated an idempotent producer.
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956307707
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-9] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-9] ProducerId set to 45 with epoch 0
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-9] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-9 unregistered
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-7
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:31:47 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956307752
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-7, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-7, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-7, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-7, groupId=test] (Re-)joining group
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-7, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-7-2fe7681b-df50-4a99-836f-1dd1f5e17d5b
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-7, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-7, groupId=test] (Re-)joining group
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-7, groupId=test] Successfully joined group with generation Generation{generationId=13, memberId='consumer-test-7-2fe7681b-df50-4a99-836f-1dd1f5e17d5b', protocol='range'}
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-7, groupId=test] Finished assignment for group at generation 13: {consumer-test-7-2fe7681b-df50-4a99-836f-1dd1f5e17d5b=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-7, groupId=test] Successfully synced group in generation Generation{generationId=13, memberId='consumer-test-7-2fe7681b-df50-4a99-836f-1dd1f5e17d5b', protocol='range'}
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-7, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-7, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-7, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=6, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-7, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-7, groupId=test] Member consumer-test-7-2fe7681b-df50-4a99-836f-1dd1f5e17d5b sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-7, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:31:47 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-7, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-7 unregistered
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-10
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:31:48 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = false
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-10] Instantiated an idempotent producer.
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956308336
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-10] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-10] ProducerId set to 46 with epoch 0
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-10] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-10 unregistered
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-8
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:31:48 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956308417
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-8, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-8, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-8, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-8, groupId=test] (Re-)joining group
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-8, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-8-5fe9f06c-4edb-4b9d-80d2-8038a78d90a6
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-8, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-8, groupId=test] (Re-)joining group
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-8, groupId=test] Successfully joined group with generation Generation{generationId=15, memberId='consumer-test-8-5fe9f06c-4edb-4b9d-80d2-8038a78d90a6', protocol='range'}
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-8, groupId=test] Finished assignment for group at generation 15: {consumer-test-8-5fe9f06c-4edb-4b9d-80d2-8038a78d90a6=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-8, groupId=test] Successfully synced group in generation Generation{generationId=15, memberId='consumer-test-8-5fe9f06c-4edb-4b9d-80d2-8038a78d90a6', protocol='range'}
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-8, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-8, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-8, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=7, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(1564) -[Consumer clientId=consumer-test-8, groupId=test] Seeking to offset 8 for partition yang.tests-0
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-8, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-8, groupId=test] Member consumer-test-8-5fe9f06c-4edb-4b9d-80d2-8038a78d90a6 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-8, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-8, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-8 unregistered
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-11
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:31:48 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-11] Instantiated an idempotent producer.
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956308473
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-11] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-11] ProducerId set to 47 with epoch 0
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-11] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-11 unregistered
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-9
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:31:48 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956308546
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-9, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-9, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-9, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-9, groupId=test] (Re-)joining group
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-9, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-9-e8ea5c36-bb91-4925-9d7e-d278d3315d57
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-9, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-9, groupId=test] (Re-)joining group
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-9, groupId=test] Successfully joined group with generation Generation{generationId=17, memberId='consumer-test-9-e8ea5c36-bb91-4925-9d7e-d278d3315d57', protocol='range'}
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-9, groupId=test] Finished assignment for group at generation 17: {consumer-test-9-e8ea5c36-bb91-4925-9d7e-d278d3315d57=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-9, groupId=test] Successfully synced group in generation Generation{generationId=17, memberId='consumer-test-9-e8ea5c36-bb91-4925-9d7e-d278d3315d57', protocol='range'}
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-9, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-9, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-9, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-9, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-9, groupId=test] Member consumer-test-9-e8ea5c36-bb91-4925-9d7e-d278d3315d57 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-9, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:31:48 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-9, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-9 unregistered
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-12
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:31:49 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-12] Instantiated an idempotent producer.
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956309126
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-12] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-12] ProducerId set to 48 with epoch 0
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-12] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-12 unregistered
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-13
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:31:49 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = false
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-13] Instantiated an idempotent producer.
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956309169
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-13] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-13] ProducerId set to 49 with epoch 0
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-13] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-13 unregistered
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-10
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:31:49 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956309198
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-10, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-10, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-10, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-10, groupId=test] (Re-)joining group
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-10, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-10-17ed977c-e242-4ab8-9a34-acad69bd2ce3
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-10, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-10, groupId=test] (Re-)joining group
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-10, groupId=test] Successfully joined group with generation Generation{generationId=19, memberId='consumer-test-10-17ed977c-e242-4ab8-9a34-acad69bd2ce3', protocol='range'}
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-10, groupId=test] Finished assignment for group at generation 19: {consumer-test-10-17ed977c-e242-4ab8-9a34-acad69bd2ce3=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-10, groupId=test] Successfully synced group in generation Generation{generationId=19, memberId='consumer-test-10-17ed977c-e242-4ab8-9a34-acad69bd2ce3', protocol='range'}
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-10, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-10, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-10, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=9, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(1564) -[Consumer clientId=consumer-test-10, groupId=test] Seeking to offset 10 for partition yang.tests-0
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-10, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-10, groupId=test] Member consumer-test-10-17ed977c-e242-4ab8-9a34-acad69bd2ce3 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-10, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-10, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-10 unregistered
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.admin.AdminClientConfig -AbstractConfig.java(370) -AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = java-admin-client
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956309238
  2024-08-29 20:31:49 [ WARN] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(68) -Error registering AppInfo mbean
  javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=java-admin-client
	at java.management/com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:436)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1855)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:955)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:890)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:320)
	at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:64)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:570)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:514)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:474)
	at org.apache.kafka.clients.admin.Admin.create(Admin.java:134)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:39)
	at com.insa.AppTest.cleanUpKafka(AppTest.java:171)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptLifecycleMethod(TimeoutExtension.java:128)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptBeforeAllMethod(TimeoutExtension.java:70)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeBeforeAllMethods$13(ClassBasedTestDescriptor.java:411)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeBeforeAllMethods(ClassBasedTestDescriptor.java:409)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:215)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:84)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:148)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1541)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
	at com.intellij.junit5.JUnit5IdeaTestRunner.startRunnerWithArgs(JUnit5IdeaTestRunner.java:57)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
	at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:232)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:55)
2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-14
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:31:49 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-14] Instantiated an idempotent producer.
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956309375
  2024-08-29 20:31:49 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-14] Error while fetching metadata with correlation id 1 : {yang.tests=LEADER_NOT_AVAILABLE}
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-14] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-14] ProducerId set to 50 with epoch 0
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-14] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-14 unregistered
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-11
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:31:49 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956309551
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-11, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-11, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-11, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-11, groupId=test] (Re-)joining group
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-11, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-11-0f80547e-0716-4a15-a56b-71c90b9730b0
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-11, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-11, groupId=test] (Re-)joining group
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-11, groupId=test] Successfully joined group with generation Generation{generationId=1, memberId='consumer-test-11-0f80547e-0716-4a15-a56b-71c90b9730b0', protocol='range'}
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-11, groupId=test] Finished assignment for group at generation 1: {consumer-test-11-0f80547e-0716-4a15-a56b-71c90b9730b0=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-11, groupId=test] Successfully synced group in generation Generation{generationId=1, memberId='consumer-test-11-0f80547e-0716-4a15-a56b-71c90b9730b0', protocol='range'}
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-11, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-11, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(1578) -[Consumer clientId=consumer-test-11, groupId=test] Found no committed offset for partition yang.tests-0
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.SubscriptionState -SubscriptionState.java(398) -[Consumer clientId=consumer-test-11, groupId=test] Resetting offset for partition yang.tests-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}.
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-11, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-11, groupId=test] Member consumer-test-11-0f80547e-0716-4a15-a56b-71c90b9730b0 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-11, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:31:49 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-11, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-11 unregistered
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-15
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:31:50 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-15] Instantiated an idempotent producer.
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956310130
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-15] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-15] ProducerId set to 51 with epoch 0
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-15] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-15 unregistered
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-12
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:31:50 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956310273
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-12, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-12, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-12, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-12, groupId=test] (Re-)joining group
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-12, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-12-e926844e-c3a2-496f-8f8f-bc9d88ef673c
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-12, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-12, groupId=test] (Re-)joining group
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-12, groupId=test] Successfully joined group with generation Generation{generationId=3, memberId='consumer-test-12-e926844e-c3a2-496f-8f8f-bc9d88ef673c', protocol='range'}
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-12, groupId=test] Finished assignment for group at generation 3: {consumer-test-12-e926844e-c3a2-496f-8f8f-bc9d88ef673c=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-12, groupId=test] Successfully synced group in generation Generation{generationId=3, memberId='consumer-test-12-e926844e-c3a2-496f-8f8f-bc9d88ef673c', protocol='range'}
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-12, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-12, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-12, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-12, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-12, groupId=test] Member consumer-test-12-e926844e-c3a2-496f-8f8f-bc9d88ef673c sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-12, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-12, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-12 unregistered
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-16
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:31:50 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-16] Instantiated an idempotent producer.
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956310879
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-16] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-16] ProducerId set to 52 with epoch 0
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-16] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-16 unregistered
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-13
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:31:50 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956310929
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-13, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-13, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-13, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-13, groupId=test] (Re-)joining group
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-13, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-13-af501156-e350-4081-8cb0-cef99c6a5896
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-13, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-13, groupId=test] (Re-)joining group
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-13, groupId=test] Successfully joined group with generation Generation{generationId=5, memberId='consumer-test-13-af501156-e350-4081-8cb0-cef99c6a5896', protocol='range'}
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-13, groupId=test] Finished assignment for group at generation 5: {consumer-test-13-af501156-e350-4081-8cb0-cef99c6a5896=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-13, groupId=test] Successfully synced group in generation Generation{generationId=5, memberId='consumer-test-13-af501156-e350-4081-8cb0-cef99c6a5896', protocol='range'}
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-13, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-13, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-13, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-13, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-13, groupId=test] Member consumer-test-13-af501156-e350-4081-8cb0-cef99c6a5896 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-13, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:31:50 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-13, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-13 unregistered
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-17
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:31:51 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-17] Instantiated an idempotent producer.
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956311501
  2024-08-29 20:31:51 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-17] Error while fetching metadata with correlation id 1 : {yang.tests.error=LEADER_NOT_AVAILABLE}
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-17] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-17] ProducerId set to 53 with epoch 0
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-17] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-17 unregistered
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-18
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:31:51 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = false
	yang.json.fail.unknown.properties = true

  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-18] Instantiated an idempotent producer.
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956311702
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-18] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-18] ProducerId set to 54 with epoch 0
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-18] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-18 unregistered
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-14
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:31:51 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956311764
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-14, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-14, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-14, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-14, groupId=test] (Re-)joining group
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-14, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-14-b84758f2-fc68-4ad6-8e80-089fa2fec08a
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-14, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-14, groupId=test] (Re-)joining group
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-14, groupId=test] Successfully joined group with generation Generation{generationId=7, memberId='consumer-test-14-b84758f2-fc68-4ad6-8e80-089fa2fec08a', protocol='range'}
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-14, groupId=test] Finished assignment for group at generation 7: {consumer-test-14-b84758f2-fc68-4ad6-8e80-089fa2fec08a=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-14, groupId=test] Successfully synced group in generation Generation{generationId=7, memberId='consumer-test-14-b84758f2-fc68-4ad6-8e80-089fa2fec08a', protocol='range'}
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-14, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-14, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-14, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(1564) -[Consumer clientId=consumer-test-14, groupId=test] Seeking to offset 4 for partition yang.tests-0
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-14, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-14, groupId=test] Member consumer-test-14-b84758f2-fc68-4ad6-8e80-089fa2fec08a sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-14, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-14, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-14 unregistered
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-19
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:31:51 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-19] Instantiated an idempotent producer.
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956311807
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-19] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-19] ProducerId set to 55 with epoch 0
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-19] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-19 unregistered
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-15
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:31:51 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956311840
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-15, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-15, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-15, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-15, groupId=test] (Re-)joining group
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-15, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-15-5308a8ca-86f7-4b47-a7fe-4bbde3eef5ba
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-15, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-15, groupId=test] (Re-)joining group
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-15, groupId=test] Successfully joined group with generation Generation{generationId=9, memberId='consumer-test-15-5308a8ca-86f7-4b47-a7fe-4bbde3eef5ba', protocol='range'}
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-15, groupId=test] Finished assignment for group at generation 9: {consumer-test-15-5308a8ca-86f7-4b47-a7fe-4bbde3eef5ba=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-15, groupId=test] Successfully synced group in generation Generation{generationId=9, memberId='consumer-test-15-5308a8ca-86f7-4b47-a7fe-4bbde3eef5ba', protocol='range'}
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-15, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-15, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-15, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=4, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-15, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-15, groupId=test] Member consumer-test-15-5308a8ca-86f7-4b47-a7fe-4bbde3eef5ba sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-15, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:31:51 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-15, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-15 unregistered
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-20
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:31:52 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-20] Instantiated an idempotent producer.
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956312415
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-20] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-20] ProducerId set to 56 with epoch 0
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-20] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-20 unregistered
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-16
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:31:52 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956312504
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-16, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-16, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-16, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-16, groupId=test] (Re-)joining group
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-16, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-16-f2cd885c-4055-4d62-86f6-095c60f55b7f
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-16, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-16, groupId=test] (Re-)joining group
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-16, groupId=test] Successfully joined group with generation Generation{generationId=11, memberId='consumer-test-16-f2cd885c-4055-4d62-86f6-095c60f55b7f', protocol='range'}
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-16, groupId=test] Finished assignment for group at generation 11: {consumer-test-16-f2cd885c-4055-4d62-86f6-095c60f55b7f=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-16, groupId=test] Successfully synced group in generation Generation{generationId=11, memberId='consumer-test-16-f2cd885c-4055-4d62-86f6-095c60f55b7f', protocol='range'}
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-16, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-16, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-16, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=5, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-16, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-16, groupId=test] Member consumer-test-16-f2cd885c-4055-4d62-86f6-095c60f55b7f sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-16, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:31:52 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-16, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-16 unregistered
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-21
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:31:53 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-21] Instantiated an idempotent producer.
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956313072
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-21] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-21] ProducerId set to 57 with epoch 0
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-21] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-21 unregistered
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-17
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:31:53 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956313094
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-17, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-17, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-17, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-17, groupId=test] (Re-)joining group
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-17, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-17-4c476e47-1235-4cb8-a53a-afa09c23f312
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-17, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-17, groupId=test] (Re-)joining group
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-17, groupId=test] Successfully joined group with generation Generation{generationId=13, memberId='consumer-test-17-4c476e47-1235-4cb8-a53a-afa09c23f312', protocol='range'}
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-17, groupId=test] Finished assignment for group at generation 13: {consumer-test-17-4c476e47-1235-4cb8-a53a-afa09c23f312=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-17, groupId=test] Successfully synced group in generation Generation{generationId=13, memberId='consumer-test-17-4c476e47-1235-4cb8-a53a-afa09c23f312', protocol='range'}
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-17, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-17, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-17, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=6, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-17, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-17, groupId=test] Member consumer-test-17-4c476e47-1235-4cb8-a53a-afa09c23f312 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-17, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-17, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-17 unregistered
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-22
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:31:53 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-22] Instantiated an idempotent producer.
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956313655
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-22] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-22] ProducerId set to 58 with epoch 0
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-22] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-22 unregistered
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-23
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:31:53 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-23] Instantiated an idempotent producer.
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956313700
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-23] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-23] ProducerId set to 59 with epoch 0
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-23] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-23 unregistered
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-18
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:31:53 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956313730
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-18, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-18, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-18, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-18, groupId=test] (Re-)joining group
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-18, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-18-53dcfe77-389c-4a1e-9353-632cf2b4fd35
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-18, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-18, groupId=test] (Re-)joining group
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-18, groupId=test] Successfully joined group with generation Generation{generationId=15, memberId='consumer-test-18-53dcfe77-389c-4a1e-9353-632cf2b4fd35', protocol='range'}
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-18, groupId=test] Finished assignment for group at generation 15: {consumer-test-18-53dcfe77-389c-4a1e-9353-632cf2b4fd35=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-18, groupId=test] Successfully synced group in generation Generation{generationId=15, memberId='consumer-test-18-53dcfe77-389c-4a1e-9353-632cf2b4fd35', protocol='range'}
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-18, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-18, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-18, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=7, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-18, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-18, groupId=test] Member consumer-test-18-53dcfe77-389c-4a1e-9353-632cf2b4fd35 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-18, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:31:53 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-18, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-18 unregistered
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-24
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:31:54 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = false
	yang.json.fail.unknown.properties = true

  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-24] Instantiated an idempotent producer.
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956314303
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-24] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-24] ProducerId set to 60 with epoch 0
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-24] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-24 unregistered
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-19
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:31:54 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956314358
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-19, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-19, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-19, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-19, groupId=test] (Re-)joining group
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-19, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-19-74d0b236-18ae-463e-8dd1-e87024e1b11b
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-19, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-19, groupId=test] (Re-)joining group
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-19, groupId=test] Successfully joined group with generation Generation{generationId=17, memberId='consumer-test-19-74d0b236-18ae-463e-8dd1-e87024e1b11b', protocol='range'}
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-19, groupId=test] Finished assignment for group at generation 17: {consumer-test-19-74d0b236-18ae-463e-8dd1-e87024e1b11b=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-19, groupId=test] Successfully synced group in generation Generation{generationId=17, memberId='consumer-test-19-74d0b236-18ae-463e-8dd1-e87024e1b11b', protocol='range'}
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-19, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-19, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-19, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=8, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(1564) -[Consumer clientId=consumer-test-19, groupId=test] Seeking to offset 9 for partition yang.tests-0
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-19, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-19, groupId=test] Member consumer-test-19-74d0b236-18ae-463e-8dd1-e87024e1b11b sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-19, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-19, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-19 unregistered
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-25
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:31:54 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-25] Instantiated an idempotent producer.
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956314402
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-25] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-25] ProducerId set to 61 with epoch 0
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-25] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-25 unregistered
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-20
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:31:54 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956314445
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-20, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-20, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-20, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-20, groupId=test] (Re-)joining group
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-20, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-20-1bbb6025-ec72-4bdb-a6b3-a7fe0b397c0d
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-20, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-20, groupId=test] (Re-)joining group
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-20, groupId=test] Successfully joined group with generation Generation{generationId=19, memberId='consumer-test-20-1bbb6025-ec72-4bdb-a6b3-a7fe0b397c0d', protocol='range'}
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-20, groupId=test] Finished assignment for group at generation 19: {consumer-test-20-1bbb6025-ec72-4bdb-a6b3-a7fe0b397c0d=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-20, groupId=test] Successfully synced group in generation Generation{generationId=19, memberId='consumer-test-20-1bbb6025-ec72-4bdb-a6b3-a7fe0b397c0d', protocol='range'}
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-20, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-20, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-20, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=9, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-20, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-20, groupId=test] Member consumer-test-20-1bbb6025-ec72-4bdb-a6b3-a7fe0b397c0d sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-20, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-20, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-20 unregistered
  2024-08-29 20:31:54 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-26
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:31:54 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-26] Instantiated an idempotent producer.
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956315014
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-26] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-26] ProducerId set to 62 with epoch 0
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-26] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-26 unregistered
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-27
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:31:55 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = false
	yang.json.fail.unknown.properties = true

  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-27] Instantiated an idempotent producer.
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956315048
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-27] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-27] ProducerId set to 63 with epoch 0
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-27] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-27 unregistered
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-21
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:31:55 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956315079
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-21, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-21, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-21, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-21, groupId=test] (Re-)joining group
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-21, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-21-9396333d-1e16-40d9-b383-112cb04ae011
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-21, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-21, groupId=test] (Re-)joining group
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-21, groupId=test] Successfully joined group with generation Generation{generationId=21, memberId='consumer-test-21-9396333d-1e16-40d9-b383-112cb04ae011', protocol='range'}
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-21, groupId=test] Finished assignment for group at generation 21: {consumer-test-21-9396333d-1e16-40d9-b383-112cb04ae011=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-21, groupId=test] Successfully synced group in generation Generation{generationId=21, memberId='consumer-test-21-9396333d-1e16-40d9-b383-112cb04ae011', protocol='range'}
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-21, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-21, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-21, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=10, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(1564) -[Consumer clientId=consumer-test-21, groupId=test] Seeking to offset 11 for partition yang.tests-0
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-21, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-21, groupId=test] Member consumer-test-21-9396333d-1e16-40d9-b383-112cb04ae011 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-21, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-21, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:31:55 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-21 unregistered
  2024-08-29 20:32:11 [ INFO] - org.apache.kafka.clients.admin.AdminClientConfig -AbstractConfig.java(370) -AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = java-admin-client
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

  2024-08-29 20:32:11 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:11 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:11 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956331821
  2024-08-29 20:32:12 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:32:12 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:32:12 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-1] Instantiated an idempotent producer.
  2024-08-29 20:32:12 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:12 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:12 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956332942
  2024-08-29 20:32:12 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {yang.tests=LEADER_NOT_AVAILABLE}
  2024-08-29 20:32:12 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-1] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:12 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-1] ProducerId set to 64 with epoch 0
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-1 unregistered
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:32:13 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956333260
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-1, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-1, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-1, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-1-b13ea508-0ba3-43a4-8c1c-2e1ca3ec13c4
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-1, groupId=test] Successfully joined group with generation Generation{generationId=1, memberId='consumer-test-1-b13ea508-0ba3-43a4-8c1c-2e1ca3ec13c4', protocol='range'}
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-1, groupId=test] Finished assignment for group at generation 1: {consumer-test-1-b13ea508-0ba3-43a4-8c1c-2e1ca3ec13c4=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-1, groupId=test] Successfully synced group in generation Generation{generationId=1, memberId='consumer-test-1-b13ea508-0ba3-43a4-8c1c-2e1ca3ec13c4', protocol='range'}
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-1, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-1, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(1578) -[Consumer clientId=consumer-test-1, groupId=test] Found no committed offset for partition yang.tests-0
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.clients.consumer.internals.SubscriptionState -SubscriptionState.java(398) -[Consumer clientId=consumer-test-1, groupId=test] Resetting offset for partition yang.tests-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}.
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-1, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-1, groupId=test] Member consumer-test-1-b13ea508-0ba3-43a4-8c1c-2e1ca3ec13c4 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-1, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:13 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-1 unregistered
  2024-08-29 20:32:24 [ INFO] - org.apache.kafka.clients.admin.AdminClientConfig -AbstractConfig.java(370) -AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = java-admin-client
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

  2024-08-29 20:32:24 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:24 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:24 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956344717
  2024-08-29 20:32:25 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:32:25 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:32:25 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-1] Instantiated an idempotent producer.
  2024-08-29 20:32:25 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:25 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:25 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956345732
  2024-08-29 20:32:25 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {yang.tests=LEADER_NOT_AVAILABLE}
  2024-08-29 20:32:25 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-1] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:25 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-1] ProducerId set to 65 with epoch 0
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-1 unregistered
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:32:26 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956346133
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-1, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-1, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-1, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-1-e42b1f20-551f-4d7c-ad32-7d1a508df5bb
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-1, groupId=test] Successfully joined group with generation Generation{generationId=1, memberId='consumer-test-1-e42b1f20-551f-4d7c-ad32-7d1a508df5bb', protocol='range'}
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-1, groupId=test] Finished assignment for group at generation 1: {consumer-test-1-e42b1f20-551f-4d7c-ad32-7d1a508df5bb=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-1, groupId=test] Successfully synced group in generation Generation{generationId=1, memberId='consumer-test-1-e42b1f20-551f-4d7c-ad32-7d1a508df5bb', protocol='range'}
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-1, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-1, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(1578) -[Consumer clientId=consumer-test-1, groupId=test] Found no committed offset for partition yang.tests-0
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.SubscriptionState -SubscriptionState.java(398) -[Consumer clientId=consumer-test-1, groupId=test] Resetting offset for partition yang.tests-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}.
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-1, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-1, groupId=test] Member consumer-test-1-e42b1f20-551f-4d7c-ad32-7d1a508df5bb sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-1, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-1 unregistered
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:32:26 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-2] Instantiated an idempotent producer.
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956346767
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-2] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-2] ProducerId set to 66 with epoch 0
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-2 unregistered
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:32:26 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956346854
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-2, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-2, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-2, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-2, groupId=test] (Re-)joining group
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-2, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-2-51333b09-4228-4f70-9940-19cac2b36ad9
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-2, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-2, groupId=test] (Re-)joining group
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-2, groupId=test] Successfully joined group with generation Generation{generationId=3, memberId='consumer-test-2-51333b09-4228-4f70-9940-19cac2b36ad9', protocol='range'}
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-2, groupId=test] Finished assignment for group at generation 3: {consumer-test-2-51333b09-4228-4f70-9940-19cac2b36ad9=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-2, groupId=test] Successfully synced group in generation Generation{generationId=3, memberId='consumer-test-2-51333b09-4228-4f70-9940-19cac2b36ad9', protocol='range'}
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-2, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-2, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-2, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-2, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-2, groupId=test] Member consumer-test-2-51333b09-4228-4f70-9940-19cac2b36ad9 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-2, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:32:26 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-2, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-2 unregistered
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-3
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:32:27 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-3] Instantiated an idempotent producer.
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956347454
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-3] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-3] ProducerId set to 67 with epoch 0
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-3] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-3 unregistered
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:32:27 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956347495
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-3, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-3, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-3, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-3, groupId=test] (Re-)joining group
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-3, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-3-4599945a-8282-4460-ad2f-3d3507acf143
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-3, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-3, groupId=test] (Re-)joining group
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-3, groupId=test] Successfully joined group with generation Generation{generationId=5, memberId='consumer-test-3-4599945a-8282-4460-ad2f-3d3507acf143', protocol='range'}
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-3, groupId=test] Finished assignment for group at generation 5: {consumer-test-3-4599945a-8282-4460-ad2f-3d3507acf143=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-3, groupId=test] Successfully synced group in generation Generation{generationId=5, memberId='consumer-test-3-4599945a-8282-4460-ad2f-3d3507acf143', protocol='range'}
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-3, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-3, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-3, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-3, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-3, groupId=test] Member consumer-test-3-4599945a-8282-4460-ad2f-3d3507acf143 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-3, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:32:27 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-3, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-3 unregistered
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-4
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:32:28 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-4] Instantiated an idempotent producer.
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956348085
  2024-08-29 20:32:28 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-4] Error while fetching metadata with correlation id 1 : {yang.tests.error=LEADER_NOT_AVAILABLE}
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-4] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-4] ProducerId set to 68 with epoch 0
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-4] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-4 unregistered
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-5
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:32:28 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = false
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-5] Instantiated an idempotent producer.
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956348234
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-5] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-5] ProducerId set to 69 with epoch 0
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-5] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-5 unregistered
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:32:28 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956348263
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-4, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-4, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-4, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-4, groupId=test] (Re-)joining group
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-4, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-4-05ec7d6d-0194-4894-aab1-656a274f0bf3
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-4, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-4, groupId=test] (Re-)joining group
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-4, groupId=test] Successfully joined group with generation Generation{generationId=7, memberId='consumer-test-4-05ec7d6d-0194-4894-aab1-656a274f0bf3', protocol='range'}
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-4, groupId=test] Finished assignment for group at generation 7: {consumer-test-4-05ec7d6d-0194-4894-aab1-656a274f0bf3=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-4, groupId=test] Successfully synced group in generation Generation{generationId=7, memberId='consumer-test-4-05ec7d6d-0194-4894-aab1-656a274f0bf3', protocol='range'}
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-4, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-4, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-4, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(1564) -[Consumer clientId=consumer-test-4, groupId=test] Seeking to offset 4 for partition yang.tests-0
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-4, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-4, groupId=test] Member consumer-test-4-05ec7d6d-0194-4894-aab1-656a274f0bf3 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-4, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-4, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-4 unregistered
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-6
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:32:28 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-6] Instantiated an idempotent producer.
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956348309
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-6] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-6] ProducerId set to 70 with epoch 0
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-6] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-6 unregistered
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:32:28 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956348344
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-5, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-5, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-5, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-5, groupId=test] (Re-)joining group
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-5, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-5-6f60263c-570a-4fff-9e5f-3e1ecf20c28f
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-5, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-5, groupId=test] (Re-)joining group
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-5, groupId=test] Successfully joined group with generation Generation{generationId=9, memberId='consumer-test-5-6f60263c-570a-4fff-9e5f-3e1ecf20c28f', protocol='range'}
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-5, groupId=test] Finished assignment for group at generation 9: {consumer-test-5-6f60263c-570a-4fff-9e5f-3e1ecf20c28f=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-5, groupId=test] Successfully synced group in generation Generation{generationId=9, memberId='consumer-test-5-6f60263c-570a-4fff-9e5f-3e1ecf20c28f', protocol='range'}
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-5, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-5, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-5, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=4, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-5, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-5, groupId=test] Member consumer-test-5-6f60263c-570a-4fff-9e5f-3e1ecf20c28f sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-5, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-5, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-5 unregistered
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-7
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:32:28 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-7] Instantiated an idempotent producer.
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956348892
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-7] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-7] ProducerId set to 71 with epoch 0
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-7] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-7 unregistered
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:32:28 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956348929
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-6, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-6, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-6, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-6, groupId=test] (Re-)joining group
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-6, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-6-d3c3e0fa-b959-4a2a-92ae-275ad708b8ca
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-6, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-6, groupId=test] (Re-)joining group
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-6, groupId=test] Successfully joined group with generation Generation{generationId=11, memberId='consumer-test-6-d3c3e0fa-b959-4a2a-92ae-275ad708b8ca', protocol='range'}
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-6, groupId=test] Finished assignment for group at generation 11: {consumer-test-6-d3c3e0fa-b959-4a2a-92ae-275ad708b8ca=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-6, groupId=test] Successfully synced group in generation Generation{generationId=11, memberId='consumer-test-6-d3c3e0fa-b959-4a2a-92ae-275ad708b8ca', protocol='range'}
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-6, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-6, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-6, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=5, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-6, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-6, groupId=test] Member consumer-test-6-d3c3e0fa-b959-4a2a-92ae-275ad708b8ca sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-6, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:32:28 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-6, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-6 unregistered
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-8
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:32:29 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-8] Instantiated an idempotent producer.
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956349513
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-8] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-8] ProducerId set to 72 with epoch 0
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-8] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-8 unregistered
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-7
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:32:29 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956349559
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-7, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-7, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-7, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-7, groupId=test] (Re-)joining group
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-7, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-7-fcd2bbf1-2b1c-4d02-9ddb-ed078b3bdd79
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-7, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-7, groupId=test] (Re-)joining group
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-7, groupId=test] Successfully joined group with generation Generation{generationId=13, memberId='consumer-test-7-fcd2bbf1-2b1c-4d02-9ddb-ed078b3bdd79', protocol='range'}
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-7, groupId=test] Finished assignment for group at generation 13: {consumer-test-7-fcd2bbf1-2b1c-4d02-9ddb-ed078b3bdd79=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-7, groupId=test] Successfully synced group in generation Generation{generationId=13, memberId='consumer-test-7-fcd2bbf1-2b1c-4d02-9ddb-ed078b3bdd79', protocol='range'}
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-7, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-7, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-7, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=6, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-7, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-7, groupId=test] Member consumer-test-7-fcd2bbf1-2b1c-4d02-9ddb-ed078b3bdd79 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-7, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:32:29 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-7, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-7 unregistered
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-9
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:32:30 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-9] Instantiated an idempotent producer.
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956350126
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-9] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-9] ProducerId set to 73 with epoch 0
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-9] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-9 unregistered
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-10
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:32:30 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-10] Instantiated an idempotent producer.
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956350203
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-10] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-10] ProducerId set to 74 with epoch 0
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-10] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-10 unregistered
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-8
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:32:30 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956350241
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-8, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-8, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-8, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-8, groupId=test] (Re-)joining group
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-8, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-8-bc108bb4-eb59-4646-ac29-98256adc6e32
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-8, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-8, groupId=test] (Re-)joining group
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-8, groupId=test] Successfully joined group with generation Generation{generationId=15, memberId='consumer-test-8-bc108bb4-eb59-4646-ac29-98256adc6e32', protocol='range'}
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-8, groupId=test] Finished assignment for group at generation 15: {consumer-test-8-bc108bb4-eb59-4646-ac29-98256adc6e32=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-8, groupId=test] Successfully synced group in generation Generation{generationId=15, memberId='consumer-test-8-bc108bb4-eb59-4646-ac29-98256adc6e32', protocol='range'}
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-8, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-8, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-8, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=7, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-8, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-8, groupId=test] Member consumer-test-8-bc108bb4-eb59-4646-ac29-98256adc6e32 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-8, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-8, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-8 unregistered
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-11
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:32:30 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = false
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-11] Instantiated an idempotent producer.
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956350820
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-11] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-11] ProducerId set to 75 with epoch 0
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-11] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-11 unregistered
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-9
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:32:30 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956350879
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-9, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-9, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-9, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-9, groupId=test] (Re-)joining group
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-9, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-9-2ff91600-d438-4e65-b76c-9b8843525813
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-9, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-9, groupId=test] (Re-)joining group
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-9, groupId=test] Successfully joined group with generation Generation{generationId=17, memberId='consumer-test-9-2ff91600-d438-4e65-b76c-9b8843525813', protocol='range'}
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-9, groupId=test] Finished assignment for group at generation 17: {consumer-test-9-2ff91600-d438-4e65-b76c-9b8843525813=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-9, groupId=test] Successfully synced group in generation Generation{generationId=17, memberId='consumer-test-9-2ff91600-d438-4e65-b76c-9b8843525813', protocol='range'}
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-9, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-9, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-9, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=8, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(1564) -[Consumer clientId=consumer-test-9, groupId=test] Seeking to offset 9 for partition yang.tests-0
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-9, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-9, groupId=test] Member consumer-test-9-2ff91600-d438-4e65-b76c-9b8843525813 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-9, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-9, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-9 unregistered
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-12
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:32:30 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-12] Instantiated an idempotent producer.
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956350923
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-12] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-12] ProducerId set to 76 with epoch 0
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-12] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-12 unregistered
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-10
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:32:30 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956350966
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-10, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-10, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-10, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-10, groupId=test] (Re-)joining group
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-10, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-10-1d9048e4-993f-4319-9cd1-e58709251526
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-10, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-10, groupId=test] (Re-)joining group
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-10, groupId=test] Successfully joined group with generation Generation{generationId=19, memberId='consumer-test-10-1d9048e4-993f-4319-9cd1-e58709251526', protocol='range'}
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-10, groupId=test] Finished assignment for group at generation 19: {consumer-test-10-1d9048e4-993f-4319-9cd1-e58709251526=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-10, groupId=test] Successfully synced group in generation Generation{generationId=19, memberId='consumer-test-10-1d9048e4-993f-4319-9cd1-e58709251526', protocol='range'}
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-10, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-10, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-10, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=9, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-10, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-10, groupId=test] Member consumer-test-10-1d9048e4-993f-4319-9cd1-e58709251526 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-10, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:32:30 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-10, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-10 unregistered
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-13
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:32:31 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-13] Instantiated an idempotent producer.
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956351547
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-13] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-13] ProducerId set to 77 with epoch 0
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-13] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-13 unregistered
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-14
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:32:31 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = false
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-14] Instantiated an idempotent producer.
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956351638
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-14] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-14] ProducerId set to 78 with epoch 0
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-14] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-14 unregistered
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-11
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:32:31 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956351670
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-11, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-11, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-11, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-11, groupId=test] (Re-)joining group
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-11, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-11-4125464a-a39b-432c-b446-c8f9ca9f0713
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-11, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-11, groupId=test] (Re-)joining group
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-11, groupId=test] Successfully joined group with generation Generation{generationId=21, memberId='consumer-test-11-4125464a-a39b-432c-b446-c8f9ca9f0713', protocol='range'}
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-11, groupId=test] Finished assignment for group at generation 21: {consumer-test-11-4125464a-a39b-432c-b446-c8f9ca9f0713=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-11, groupId=test] Successfully synced group in generation Generation{generationId=21, memberId='consumer-test-11-4125464a-a39b-432c-b446-c8f9ca9f0713', protocol='range'}
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-11, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-11, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-11, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=10, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(1564) -[Consumer clientId=consumer-test-11, groupId=test] Seeking to offset 11 for partition yang.tests-0
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-11, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-11, groupId=test] Member consumer-test-11-4125464a-a39b-432c-b446-c8f9ca9f0713 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-11, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-11, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-11 unregistered
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.admin.AdminClientConfig -AbstractConfig.java(370) -AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = java-admin-client
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956351707
  2024-08-29 20:32:31 [ WARN] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(68) -Error registering AppInfo mbean
  javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=java-admin-client
	at java.management/com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:436)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1855)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:955)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:890)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:320)
	at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:64)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:570)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:514)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:474)
	at org.apache.kafka.clients.admin.Admin.create(Admin.java:134)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:39)
	at com.insa.AppTest.cleanUpKafka(AppTest.java:171)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptLifecycleMethod(TimeoutExtension.java:128)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptBeforeAllMethod(TimeoutExtension.java:70)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeBeforeAllMethods$13(ClassBasedTestDescriptor.java:411)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeBeforeAllMethods(ClassBasedTestDescriptor.java:409)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:215)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:84)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:148)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1541)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
	at com.intellij.junit5.JUnit5IdeaTestRunner.startRunnerWithArgs(JUnit5IdeaTestRunner.java:57)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
	at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:232)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:55)
2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-15
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:32:31 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-15] Instantiated an idempotent producer.
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956351825
  2024-08-29 20:32:31 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-15] Error while fetching metadata with correlation id 1 : {yang.tests=LEADER_NOT_AVAILABLE}
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-15] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-15] ProducerId set to 79 with epoch 0
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-15] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-15 unregistered
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-12
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:32:31 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956351983
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-12, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-12, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-12, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-12, groupId=test] (Re-)joining group
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-12, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-12-39496c74-80ce-4c4d-980b-f0c066a16c5b
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-12, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-12, groupId=test] (Re-)joining group
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-12, groupId=test] Successfully joined group with generation Generation{generationId=1, memberId='consumer-test-12-39496c74-80ce-4c4d-980b-f0c066a16c5b', protocol='range'}
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-12, groupId=test] Finished assignment for group at generation 1: {consumer-test-12-39496c74-80ce-4c4d-980b-f0c066a16c5b=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-12, groupId=test] Successfully synced group in generation Generation{generationId=1, memberId='consumer-test-12-39496c74-80ce-4c4d-980b-f0c066a16c5b', protocol='range'}
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-12, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-12, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(1578) -[Consumer clientId=consumer-test-12, groupId=test] Found no committed offset for partition yang.tests-0
  2024-08-29 20:32:31 [ INFO] - org.apache.kafka.clients.consumer.internals.SubscriptionState -SubscriptionState.java(398) -[Consumer clientId=consumer-test-12, groupId=test] Resetting offset for partition yang.tests-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}.
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-12, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-12, groupId=test] Member consumer-test-12-39496c74-80ce-4c4d-980b-f0c066a16c5b sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-12, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-12, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-12 unregistered
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-16
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:32:32 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-16] Instantiated an idempotent producer.
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956352621
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-16] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-16] ProducerId set to 80 with epoch 0
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-16] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-16 unregistered
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-13
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:32:32 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956352697
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-13, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-13, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-13, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-13, groupId=test] (Re-)joining group
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-13, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-13-3fb5d8fa-23fd-42d4-8c18-5acb7800c334
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-13, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-13, groupId=test] (Re-)joining group
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-13, groupId=test] Successfully joined group with generation Generation{generationId=3, memberId='consumer-test-13-3fb5d8fa-23fd-42d4-8c18-5acb7800c334', protocol='range'}
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-13, groupId=test] Finished assignment for group at generation 3: {consumer-test-13-3fb5d8fa-23fd-42d4-8c18-5acb7800c334=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-13, groupId=test] Successfully synced group in generation Generation{generationId=3, memberId='consumer-test-13-3fb5d8fa-23fd-42d4-8c18-5acb7800c334', protocol='range'}
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-13, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-13, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-13, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-13, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-13, groupId=test] Member consumer-test-13-3fb5d8fa-23fd-42d4-8c18-5acb7800c334 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-13, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:32:32 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-13, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-13 unregistered
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-17
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:32:33 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-17] Instantiated an idempotent producer.
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956353269
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-17] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-17] ProducerId set to 81 with epoch 0
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-17] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-17 unregistered
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-14
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:32:33 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956353313
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-14, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-14, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-14, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-14, groupId=test] (Re-)joining group
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-14, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-14-ef4b81ce-b9c8-4e51-923f-4f1e888b455c
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-14, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-14, groupId=test] (Re-)joining group
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-14, groupId=test] Successfully joined group with generation Generation{generationId=5, memberId='consumer-test-14-ef4b81ce-b9c8-4e51-923f-4f1e888b455c', protocol='range'}
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-14, groupId=test] Finished assignment for group at generation 5: {consumer-test-14-ef4b81ce-b9c8-4e51-923f-4f1e888b455c=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-14, groupId=test] Successfully synced group in generation Generation{generationId=5, memberId='consumer-test-14-ef4b81ce-b9c8-4e51-923f-4f1e888b455c', protocol='range'}
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-14, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-14, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-14, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-14, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-14, groupId=test] Member consumer-test-14-ef4b81ce-b9c8-4e51-923f-4f1e888b455c sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-14, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-14, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-14 unregistered
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-18
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:32:33 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-18] Instantiated an idempotent producer.
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956353861
  2024-08-29 20:32:33 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-18] Error while fetching metadata with correlation id 1 : {yang.tests.error=LEADER_NOT_AVAILABLE}
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-18] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-18] ProducerId set to 82 with epoch 0
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-18] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:33 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-18 unregistered
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-19
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:32:34 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = false
	yang.json.fail.unknown.properties = true

  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-19] Instantiated an idempotent producer.
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956354036
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-19] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-19] ProducerId set to 83 with epoch 0
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-19] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-19 unregistered
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-15
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:32:34 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956354102
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-15, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-15, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-15, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-15, groupId=test] (Re-)joining group
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-15, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-15-418bd998-42d9-41a2-9ec3-40f03f0b9b10
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-15, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-15, groupId=test] (Re-)joining group
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-15, groupId=test] Successfully joined group with generation Generation{generationId=7, memberId='consumer-test-15-418bd998-42d9-41a2-9ec3-40f03f0b9b10', protocol='range'}
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-15, groupId=test] Finished assignment for group at generation 7: {consumer-test-15-418bd998-42d9-41a2-9ec3-40f03f0b9b10=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-15, groupId=test] Successfully synced group in generation Generation{generationId=7, memberId='consumer-test-15-418bd998-42d9-41a2-9ec3-40f03f0b9b10', protocol='range'}
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-15, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-15, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-15, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(1564) -[Consumer clientId=consumer-test-15, groupId=test] Seeking to offset 4 for partition yang.tests-0
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-15, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-15, groupId=test] Member consumer-test-15-418bd998-42d9-41a2-9ec3-40f03f0b9b10 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-15, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-15, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-15 unregistered
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-20
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:32:34 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-20] Instantiated an idempotent producer.
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956354143
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-20] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-20] ProducerId set to 84 with epoch 0
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-20] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-20 unregistered
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-16
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:32:34 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956354173
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-16, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-16, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-16, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-16, groupId=test] (Re-)joining group
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-16, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-16-35c96135-c67b-492b-97c2-27c44425cc3c
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-16, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-16, groupId=test] (Re-)joining group
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-16, groupId=test] Successfully joined group with generation Generation{generationId=9, memberId='consumer-test-16-35c96135-c67b-492b-97c2-27c44425cc3c', protocol='range'}
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-16, groupId=test] Finished assignment for group at generation 9: {consumer-test-16-35c96135-c67b-492b-97c2-27c44425cc3c=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-16, groupId=test] Successfully synced group in generation Generation{generationId=9, memberId='consumer-test-16-35c96135-c67b-492b-97c2-27c44425cc3c', protocol='range'}
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-16, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-16, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-16, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=4, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-16, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-16, groupId=test] Member consumer-test-16-35c96135-c67b-492b-97c2-27c44425cc3c sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-16, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-16, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-16 unregistered
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-21
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:32:34 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-21] Instantiated an idempotent producer.
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956354731
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-21] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-21] ProducerId set to 85 with epoch 0
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-21] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-21 unregistered
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-17
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:32:34 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956354769
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-17, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-17, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-17, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-17, groupId=test] (Re-)joining group
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-17, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-17-68acb9e1-bfc5-46dc-9023-b59b1ee688b6
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-17, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-17, groupId=test] (Re-)joining group
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-17, groupId=test] Successfully joined group with generation Generation{generationId=11, memberId='consumer-test-17-68acb9e1-bfc5-46dc-9023-b59b1ee688b6', protocol='range'}
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-17, groupId=test] Finished assignment for group at generation 11: {consumer-test-17-68acb9e1-bfc5-46dc-9023-b59b1ee688b6=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-17, groupId=test] Successfully synced group in generation Generation{generationId=11, memberId='consumer-test-17-68acb9e1-bfc5-46dc-9023-b59b1ee688b6', protocol='range'}
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-17, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-17, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-17, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=5, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-17, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-17, groupId=test] Member consumer-test-17-68acb9e1-bfc5-46dc-9023-b59b1ee688b6 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-17, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:32:34 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-17, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-17 unregistered
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-22
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:32:35 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-22] Instantiated an idempotent producer.
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956355318
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-22] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-22] ProducerId set to 86 with epoch 0
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-22] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-22 unregistered
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-18
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:32:35 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956355349
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-18, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-18, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-18, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-18, groupId=test] (Re-)joining group
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-18, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-18-49b0e875-f0a7-4519-bdf2-b3a06a73e617
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-18, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-18, groupId=test] (Re-)joining group
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-18, groupId=test] Successfully joined group with generation Generation{generationId=13, memberId='consumer-test-18-49b0e875-f0a7-4519-bdf2-b3a06a73e617', protocol='range'}
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-18, groupId=test] Finished assignment for group at generation 13: {consumer-test-18-49b0e875-f0a7-4519-bdf2-b3a06a73e617=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-18, groupId=test] Successfully synced group in generation Generation{generationId=13, memberId='consumer-test-18-49b0e875-f0a7-4519-bdf2-b3a06a73e617', protocol='range'}
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-18, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-18, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-18, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=6, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-18, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-18, groupId=test] Member consumer-test-18-49b0e875-f0a7-4519-bdf2-b3a06a73e617 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-18, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-18, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-18 unregistered
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-23
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:32:35 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-23] Instantiated an idempotent producer.
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956355886
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-23] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-23] ProducerId set to 87 with epoch 0
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-23] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-23 unregistered
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-24
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:32:35 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-24] Instantiated an idempotent producer.
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956355914
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-24] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-24] ProducerId set to 88 with epoch 0
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-24] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-24 unregistered
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-19
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:32:35 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956355938
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-19, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-19, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-19, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-19, groupId=test] (Re-)joining group
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-19, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-19-c13842c4-1e9a-486c-8489-3fde55356d14
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-19, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-19, groupId=test] (Re-)joining group
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-19, groupId=test] Successfully joined group with generation Generation{generationId=15, memberId='consumer-test-19-c13842c4-1e9a-486c-8489-3fde55356d14', protocol='range'}
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-19, groupId=test] Finished assignment for group at generation 15: {consumer-test-19-c13842c4-1e9a-486c-8489-3fde55356d14=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-19, groupId=test] Successfully synced group in generation Generation{generationId=15, memberId='consumer-test-19-c13842c4-1e9a-486c-8489-3fde55356d14', protocol='range'}
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-19, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-19, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-19, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=7, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-19, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-19, groupId=test] Member consumer-test-19-c13842c4-1e9a-486c-8489-3fde55356d14 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-19, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:32:35 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-19, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-19 unregistered
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-25
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:32:36 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = false
	yang.json.fail.unknown.properties = true

  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-25] Instantiated an idempotent producer.
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956356498
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-25] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-25] ProducerId set to 89 with epoch 0
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-25] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-25 unregistered
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-20
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:32:36 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956356570
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-20, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-20, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-20, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-20, groupId=test] (Re-)joining group
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-20, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-20-c556fba9-ee29-4558-a6ff-30119d079887
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-20, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-20, groupId=test] (Re-)joining group
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-20, groupId=test] Successfully joined group with generation Generation{generationId=17, memberId='consumer-test-20-c556fba9-ee29-4558-a6ff-30119d079887', protocol='range'}
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-20, groupId=test] Finished assignment for group at generation 17: {consumer-test-20-c556fba9-ee29-4558-a6ff-30119d079887=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-20, groupId=test] Successfully synced group in generation Generation{generationId=17, memberId='consumer-test-20-c556fba9-ee29-4558-a6ff-30119d079887', protocol='range'}
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-20, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-20, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-20, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=8, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(1564) -[Consumer clientId=consumer-test-20, groupId=test] Seeking to offset 9 for partition yang.tests-0
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-20, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-20, groupId=test] Member consumer-test-20-c556fba9-ee29-4558-a6ff-30119d079887 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-20, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-20, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-20 unregistered
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-26
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:32:36 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-26] Instantiated an idempotent producer.
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956356621
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-26] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-26] ProducerId set to 90 with epoch 0
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-26] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-26 unregistered
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-21
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:32:36 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956356674
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-21, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-21, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-21, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-21, groupId=test] (Re-)joining group
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-21, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-21-c14f9c18-3314-4172-96ad-dfca0d69a2f5
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-21, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-21, groupId=test] (Re-)joining group
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-21, groupId=test] Successfully joined group with generation Generation{generationId=19, memberId='consumer-test-21-c14f9c18-3314-4172-96ad-dfca0d69a2f5', protocol='range'}
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-21, groupId=test] Finished assignment for group at generation 19: {consumer-test-21-c14f9c18-3314-4172-96ad-dfca0d69a2f5=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-21, groupId=test] Successfully synced group in generation Generation{generationId=19, memberId='consumer-test-21-c14f9c18-3314-4172-96ad-dfca0d69a2f5', protocol='range'}
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-21, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-21, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-21, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=9, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-21, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-21, groupId=test] Member consumer-test-21-c14f9c18-3314-4172-96ad-dfca0d69a2f5 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-21, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:32:36 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-21, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-21 unregistered
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-27
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:32:37 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-27] Instantiated an idempotent producer.
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956357235
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-27] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-27] ProducerId set to 91 with epoch 0
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-27] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-27 unregistered
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-28
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:32:37 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = false
	yang.json.fail.unknown.properties = true

  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-28] Instantiated an idempotent producer.
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956357328
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-28] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-28] ProducerId set to 92 with epoch 0
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-28] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-28 unregistered
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-22
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:32:37 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956357366
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-22, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-22, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-22, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-22, groupId=test] (Re-)joining group
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-22, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-22-5fbaa99f-32da-47b8-b5c5-ae2de99d055b
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-22, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-22, groupId=test] (Re-)joining group
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-22, groupId=test] Successfully joined group with generation Generation{generationId=21, memberId='consumer-test-22-5fbaa99f-32da-47b8-b5c5-ae2de99d055b', protocol='range'}
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-22, groupId=test] Finished assignment for group at generation 21: {consumer-test-22-5fbaa99f-32da-47b8-b5c5-ae2de99d055b=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-22, groupId=test] Successfully synced group in generation Generation{generationId=21, memberId='consumer-test-22-5fbaa99f-32da-47b8-b5c5-ae2de99d055b', protocol='range'}
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-22, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-22, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-22, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=10, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(1564) -[Consumer clientId=consumer-test-22, groupId=test] Seeking to offset 11 for partition yang.tests-0
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-22, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-22, groupId=test] Member consumer-test-22-5fbaa99f-32da-47b8-b5c5-ae2de99d055b sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-22, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-22, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:37 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-22 unregistered
  2024-08-29 20:32:57 [ INFO] - org.apache.kafka.clients.admin.AdminClientConfig -AbstractConfig.java(370) -AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = java-admin-client
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

  2024-08-29 20:32:57 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:57 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:57 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956377469
  2024-08-29 20:32:58 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:32:58 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:32:58 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-1] Instantiated an idempotent producer.
  2024-08-29 20:32:58 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:58 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:58 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956378760
  2024-08-29 20:32:58 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {yang.tests=LEADER_NOT_AVAILABLE}
  2024-08-29 20:32:58 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-1] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:58 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-1] ProducerId set to 93 with epoch 0
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-1 unregistered
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:32:59 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956379249
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-1, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-1, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-1, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-1-4e0e6fc0-8dc4-4f7a-93c7-27b07d35d69a
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-1, groupId=test] (Re-)joining group
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-1, groupId=test] Successfully joined group with generation Generation{generationId=1, memberId='consumer-test-1-4e0e6fc0-8dc4-4f7a-93c7-27b07d35d69a', protocol='range'}
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-1, groupId=test] Finished assignment for group at generation 1: {consumer-test-1-4e0e6fc0-8dc4-4f7a-93c7-27b07d35d69a=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-1, groupId=test] Successfully synced group in generation Generation{generationId=1, memberId='consumer-test-1-4e0e6fc0-8dc4-4f7a-93c7-27b07d35d69a', protocol='range'}
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-1, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-1, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(1578) -[Consumer clientId=consumer-test-1, groupId=test] Found no committed offset for partition yang.tests-0
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.clients.consumer.internals.SubscriptionState -SubscriptionState.java(398) -[Consumer clientId=consumer-test-1, groupId=test] Resetting offset for partition yang.tests-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}.
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-1, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-1, groupId=test] Member consumer-test-1-4e0e6fc0-8dc4-4f7a-93c7-27b07d35d69a sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-1, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-1, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-1 unregistered
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:32:59 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-2] Instantiated an idempotent producer.
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956379907
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-2] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-2] ProducerId set to 94 with epoch 0
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:32:59 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-2 unregistered
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:33:00 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956380015
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-2, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-2, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-2, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-2, groupId=test] (Re-)joining group
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-2, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-2-7c560d3f-3647-47f8-96c3-586a37f31caa
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-2, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-2, groupId=test] (Re-)joining group
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-2, groupId=test] Successfully joined group with generation Generation{generationId=3, memberId='consumer-test-2-7c560d3f-3647-47f8-96c3-586a37f31caa', protocol='range'}
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-2, groupId=test] Finished assignment for group at generation 3: {consumer-test-2-7c560d3f-3647-47f8-96c3-586a37f31caa=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-2, groupId=test] Successfully synced group in generation Generation{generationId=3, memberId='consumer-test-2-7c560d3f-3647-47f8-96c3-586a37f31caa', protocol='range'}
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-2, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-2, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-2, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-2, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-2, groupId=test] Member consumer-test-2-7c560d3f-3647-47f8-96c3-586a37f31caa sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-2, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-2, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-2 unregistered
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-3
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:33:00 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-3] Instantiated an idempotent producer.
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956380588
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-3] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-3] ProducerId set to 95 with epoch 0
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-3] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-3 unregistered
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:33:00 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956380639
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-3, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-3, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-3, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-3, groupId=test] (Re-)joining group
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-3, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-3-2af46534-39f6-4085-80e4-4a1a6a39dfde
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-3, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-3, groupId=test] (Re-)joining group
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-3, groupId=test] Successfully joined group with generation Generation{generationId=5, memberId='consumer-test-3-2af46534-39f6-4085-80e4-4a1a6a39dfde', protocol='range'}
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-3, groupId=test] Finished assignment for group at generation 5: {consumer-test-3-2af46534-39f6-4085-80e4-4a1a6a39dfde=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-3, groupId=test] Successfully synced group in generation Generation{generationId=5, memberId='consumer-test-3-2af46534-39f6-4085-80e4-4a1a6a39dfde', protocol='range'}
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-3, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-3, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-3, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-3, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-3, groupId=test] Member consumer-test-3-2af46534-39f6-4085-80e4-4a1a6a39dfde sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-3, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:33:00 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-3, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-3 unregistered
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-4
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:33:01 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-4] Instantiated an idempotent producer.
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956381223
  2024-08-29 20:33:01 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-4] Error while fetching metadata with correlation id 1 : {yang.tests.error=LEADER_NOT_AVAILABLE}
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-4] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-4] ProducerId set to 96 with epoch 0
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-4] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-4 unregistered
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-5
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:33:01 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = false
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-5] Instantiated an idempotent producer.
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956381410
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-5] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-5] ProducerId set to 97 with epoch 0
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-5] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-5 unregistered
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:33:01 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956381469
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-4, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-4, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-4, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-4, groupId=test] (Re-)joining group
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-4, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-4-24b8908f-b4e3-4b4a-bd2a-91252872576b
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-4, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-4, groupId=test] (Re-)joining group
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-4, groupId=test] Successfully joined group with generation Generation{generationId=7, memberId='consumer-test-4-24b8908f-b4e3-4b4a-bd2a-91252872576b', protocol='range'}
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-4, groupId=test] Finished assignment for group at generation 7: {consumer-test-4-24b8908f-b4e3-4b4a-bd2a-91252872576b=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-4, groupId=test] Successfully synced group in generation Generation{generationId=7, memberId='consumer-test-4-24b8908f-b4e3-4b4a-bd2a-91252872576b', protocol='range'}
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-4, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-4, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-4, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(1564) -[Consumer clientId=consumer-test-4, groupId=test] Seeking to offset 4 for partition yang.tests-0
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-4, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-4, groupId=test] Member consumer-test-4-24b8908f-b4e3-4b4a-bd2a-91252872576b sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-4, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-4, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-4 unregistered
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-6
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:33:01 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-6] Instantiated an idempotent producer.
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956381521
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-6] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-6] ProducerId set to 98 with epoch 0
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-6] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-6 unregistered
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:33:01 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956381567
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-5, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-5, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-5, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-5, groupId=test] (Re-)joining group
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-5, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-5-0ee1924c-5fa7-40a4-91f2-dd43a002314f
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-5, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-5, groupId=test] (Re-)joining group
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-5, groupId=test] Successfully joined group with generation Generation{generationId=9, memberId='consumer-test-5-0ee1924c-5fa7-40a4-91f2-dd43a002314f', protocol='range'}
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-5, groupId=test] Finished assignment for group at generation 9: {consumer-test-5-0ee1924c-5fa7-40a4-91f2-dd43a002314f=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-5, groupId=test] Successfully synced group in generation Generation{generationId=9, memberId='consumer-test-5-0ee1924c-5fa7-40a4-91f2-dd43a002314f', protocol='range'}
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-5, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-5, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-5, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=4, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-5, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-5, groupId=test] Member consumer-test-5-0ee1924c-5fa7-40a4-91f2-dd43a002314f sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-5, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:33:01 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-5, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-5 unregistered
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-7
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:33:02 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-7] Instantiated an idempotent producer.
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956382130
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-7] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-7] ProducerId set to 99 with epoch 0
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-7] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-7 unregistered
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:33:02 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956382168
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-6, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-6, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-6, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-6, groupId=test] (Re-)joining group
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-6, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-6-b126f6be-17ab-4a8d-a066-f352ea69ebbe
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-6, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-6, groupId=test] (Re-)joining group
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-6, groupId=test] Successfully joined group with generation Generation{generationId=11, memberId='consumer-test-6-b126f6be-17ab-4a8d-a066-f352ea69ebbe', protocol='range'}
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-6, groupId=test] Finished assignment for group at generation 11: {consumer-test-6-b126f6be-17ab-4a8d-a066-f352ea69ebbe=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-6, groupId=test] Successfully synced group in generation Generation{generationId=11, memberId='consumer-test-6-b126f6be-17ab-4a8d-a066-f352ea69ebbe', protocol='range'}
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-6, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-6, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-6, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=5, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-6, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-6, groupId=test] Member consumer-test-6-b126f6be-17ab-4a8d-a066-f352ea69ebbe sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-6, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-6, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-6 unregistered
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-8
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:33:02 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-8] Instantiated an idempotent producer.
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956382708
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-8] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-8] ProducerId set to 100 with epoch 0
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-8] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-8 unregistered
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-7
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:33:02 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956382733
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-7, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-7, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-7, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-7, groupId=test] (Re-)joining group
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-7, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-7-8b4853ec-4933-47cf-987c-403330abf369
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-7, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-7, groupId=test] (Re-)joining group
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-7, groupId=test] Successfully joined group with generation Generation{generationId=13, memberId='consumer-test-7-8b4853ec-4933-47cf-987c-403330abf369', protocol='range'}
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-7, groupId=test] Finished assignment for group at generation 13: {consumer-test-7-8b4853ec-4933-47cf-987c-403330abf369=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-7, groupId=test] Successfully synced group in generation Generation{generationId=13, memberId='consumer-test-7-8b4853ec-4933-47cf-987c-403330abf369', protocol='range'}
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-7, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-7, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-7, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=6, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-7, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-7, groupId=test] Member consumer-test-7-8b4853ec-4933-47cf-987c-403330abf369 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-7, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:33:02 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-7, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-7 unregistered
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-9
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:33:03 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-9] Instantiated an idempotent producer.
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956383308
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-9] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-9] ProducerId set to 101 with epoch 0
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-9] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-9 unregistered
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-10
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:33:03 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-10] Instantiated an idempotent producer.
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956383346
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-10] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-10] ProducerId set to 102 with epoch 0
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-10] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-10 unregistered
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-8
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:33:03 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956383371
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-8, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-8, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-8, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-8, groupId=test] (Re-)joining group
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-8, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-8-82c971ad-0d11-493f-8557-31af9d07f7bb
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-8, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-8, groupId=test] (Re-)joining group
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-8, groupId=test] Successfully joined group with generation Generation{generationId=15, memberId='consumer-test-8-82c971ad-0d11-493f-8557-31af9d07f7bb', protocol='range'}
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-8, groupId=test] Finished assignment for group at generation 15: {consumer-test-8-82c971ad-0d11-493f-8557-31af9d07f7bb=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-8, groupId=test] Successfully synced group in generation Generation{generationId=15, memberId='consumer-test-8-82c971ad-0d11-493f-8557-31af9d07f7bb', protocol='range'}
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-8, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-8, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-8, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=7, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-8, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-8, groupId=test] Member consumer-test-8-82c971ad-0d11-493f-8557-31af9d07f7bb sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-8, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-8, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-8 unregistered
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-11
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:33:03 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = false
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-11] Instantiated an idempotent producer.
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956383924
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-11] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-11] ProducerId set to 103 with epoch 0
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-11] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-11 unregistered
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-9
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:33:03 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956383965
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-9, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-9, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-9, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-9, groupId=test] (Re-)joining group
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-9, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-9-a33b6f90-2770-445b-9c6a-1930a10a65a5
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-9, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-9, groupId=test] (Re-)joining group
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-9, groupId=test] Successfully joined group with generation Generation{generationId=17, memberId='consumer-test-9-a33b6f90-2770-445b-9c6a-1930a10a65a5', protocol='range'}
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-9, groupId=test] Finished assignment for group at generation 17: {consumer-test-9-a33b6f90-2770-445b-9c6a-1930a10a65a5=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-9, groupId=test] Successfully synced group in generation Generation{generationId=17, memberId='consumer-test-9-a33b6f90-2770-445b-9c6a-1930a10a65a5', protocol='range'}
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-9, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-9, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-9, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=8, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(1564) -[Consumer clientId=consumer-test-9, groupId=test] Seeking to offset 9 for partition yang.tests-0
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-9, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-9, groupId=test] Member consumer-test-9-a33b6f90-2770-445b-9c6a-1930a10a65a5 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-9, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-9, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:03 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-9 unregistered
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-12
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:33:04 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-12] Instantiated an idempotent producer.
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956384012
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-12] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-12] ProducerId set to 104 with epoch 0
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-12] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-12 unregistered
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-10
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:33:04 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956384069
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-10, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-10, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-10, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-10, groupId=test] (Re-)joining group
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-10, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-10-b9310665-8ff0-46f5-951e-179be4d11b12
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-10, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-10, groupId=test] (Re-)joining group
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-10, groupId=test] Successfully joined group with generation Generation{generationId=19, memberId='consumer-test-10-b9310665-8ff0-46f5-951e-179be4d11b12', protocol='range'}
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-10, groupId=test] Finished assignment for group at generation 19: {consumer-test-10-b9310665-8ff0-46f5-951e-179be4d11b12=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-10, groupId=test] Successfully synced group in generation Generation{generationId=19, memberId='consumer-test-10-b9310665-8ff0-46f5-951e-179be4d11b12', protocol='range'}
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-10, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-10, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-10, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=9, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-10, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-10, groupId=test] Member consumer-test-10-b9310665-8ff0-46f5-951e-179be4d11b12 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-10, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-10, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-10 unregistered
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-13
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:33:04 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-13] Instantiated an idempotent producer.
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956384637
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-13] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-13] ProducerId set to 105 with epoch 0
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-13] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-13 unregistered
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-14
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializer

  2024-08-29 20:33:04 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = false
	yang.cbor.fail.unknown.properties = true

  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-14] Instantiated an idempotent producer.
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956384683
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-14] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-14] ProducerId set to 106 with epoch 0
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-14] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-14 unregistered
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-11
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializer

  2024-08-29 20:33:04 [ INFO] - com.insa.kafka.serializers.yang.cbor.KafkaYangCborSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangCborSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.cbor.fail.invalid.schema = true
	yang.cbor.fail.unknown.properties = true
	yang.cbor.key.type = class java.lang.Object
	yang.cbor.value.type = class java.lang.Object

  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956384715
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-11, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-11, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-11, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-11, groupId=test] (Re-)joining group
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-11, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-11-b2e655e3-733f-40a8-8688-910c64ea5cca
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-11, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-11, groupId=test] (Re-)joining group
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-11, groupId=test] Successfully joined group with generation Generation{generationId=21, memberId='consumer-test-11-b2e655e3-733f-40a8-8688-910c64ea5cca', protocol='range'}
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-11, groupId=test] Finished assignment for group at generation 21: {consumer-test-11-b2e655e3-733f-40a8-8688-910c64ea5cca=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-11, groupId=test] Successfully synced group in generation Generation{generationId=21, memberId='consumer-test-11-b2e655e3-733f-40a8-8688-910c64ea5cca', protocol='range'}
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-11, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-11, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-11, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=10, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(1564) -[Consumer clientId=consumer-test-11, groupId=test] Seeking to offset 11 for partition yang.tests-0
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-11, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-11, groupId=test] Member consumer-test-11-b2e655e3-733f-40a8-8688-910c64ea5cca sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-11, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-11, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-11 unregistered
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.admin.AdminClientConfig -AbstractConfig.java(370) -AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = java-admin-client
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956384756
  2024-08-29 20:33:04 [ WARN] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(68) -Error registering AppInfo mbean
  javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=java-admin-client
	at java.management/com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:436)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1855)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:955)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:890)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:320)
	at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:64)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:570)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:514)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:474)
	at org.apache.kafka.clients.admin.Admin.create(Admin.java:134)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:39)
	at com.insa.AppTest.cleanUpKafka(AppTest.java:171)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptLifecycleMethod(TimeoutExtension.java:128)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptBeforeAllMethod(TimeoutExtension.java:70)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeBeforeAllMethods$13(ClassBasedTestDescriptor.java:411)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeBeforeAllMethods(ClassBasedTestDescriptor.java:409)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:215)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:84)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:148)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1541)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
	at com.intellij.junit5.JUnit5IdeaTestRunner.startRunnerWithArgs(JUnit5IdeaTestRunner.java:57)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
	at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:232)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:55)
2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-15
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:33:04 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-15] Instantiated an idempotent producer.
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956384886
  2024-08-29 20:33:04 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-15] Error while fetching metadata with correlation id 1 : {yang.tests=LEADER_NOT_AVAILABLE}
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-15] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:04 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-15] ProducerId set to 107 with epoch 0
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-15] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-15 unregistered
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-12
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:33:05 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956385060
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-12, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-12, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-12, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-12, groupId=test] (Re-)joining group
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-12, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-12-811f7315-cea5-4c36-9187-0c937f4a2626
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-12, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-12, groupId=test] (Re-)joining group
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-12, groupId=test] Successfully joined group with generation Generation{generationId=1, memberId='consumer-test-12-811f7315-cea5-4c36-9187-0c937f4a2626', protocol='range'}
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-12, groupId=test] Finished assignment for group at generation 1: {consumer-test-12-811f7315-cea5-4c36-9187-0c937f4a2626=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-12, groupId=test] Successfully synced group in generation Generation{generationId=1, memberId='consumer-test-12-811f7315-cea5-4c36-9187-0c937f4a2626', protocol='range'}
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-12, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-12, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(1578) -[Consumer clientId=consumer-test-12, groupId=test] Found no committed offset for partition yang.tests-0
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.SubscriptionState -SubscriptionState.java(398) -[Consumer clientId=consumer-test-12, groupId=test] Resetting offset for partition yang.tests-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}.
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-12, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-12, groupId=test] Member consumer-test-12-811f7315-cea5-4c36-9187-0c937f4a2626 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-12, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-12, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-12 unregistered
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-16
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:33:05 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-16] Instantiated an idempotent producer.
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956385637
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-16] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-16] ProducerId set to 108 with epoch 0
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-16] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-16 unregistered
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-13
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:33:05 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956385742
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-13, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-13, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-13, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-13, groupId=test] (Re-)joining group
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-13, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-13-d2e95b99-7fee-498a-9dcc-031b9bc177f6
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-13, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-13, groupId=test] (Re-)joining group
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-13, groupId=test] Successfully joined group with generation Generation{generationId=3, memberId='consumer-test-13-d2e95b99-7fee-498a-9dcc-031b9bc177f6', protocol='range'}
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-13, groupId=test] Finished assignment for group at generation 3: {consumer-test-13-d2e95b99-7fee-498a-9dcc-031b9bc177f6=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-13, groupId=test] Successfully synced group in generation Generation{generationId=3, memberId='consumer-test-13-d2e95b99-7fee-498a-9dcc-031b9bc177f6', protocol='range'}
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-13, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-13, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-13, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-13, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-13, groupId=test] Member consumer-test-13-d2e95b99-7fee-498a-9dcc-031b9bc177f6 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-13, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:33:05 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-13, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-13 unregistered
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-17
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:33:06 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-17] Instantiated an idempotent producer.
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956386320
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-17] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-17] ProducerId set to 109 with epoch 0
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-17] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-17 unregistered
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-14
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:33:06 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956386407
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-14, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-14, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-14, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-14, groupId=test] (Re-)joining group
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-14, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-14-6ea23b77-bc6c-4cb6-9605-1f1ffa2962bf
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-14, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-14, groupId=test] (Re-)joining group
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-14, groupId=test] Successfully joined group with generation Generation{generationId=5, memberId='consumer-test-14-6ea23b77-bc6c-4cb6-9605-1f1ffa2962bf', protocol='range'}
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-14, groupId=test] Finished assignment for group at generation 5: {consumer-test-14-6ea23b77-bc6c-4cb6-9605-1f1ffa2962bf=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-14, groupId=test] Successfully synced group in generation Generation{generationId=5, memberId='consumer-test-14-6ea23b77-bc6c-4cb6-9605-1f1ffa2962bf', protocol='range'}
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-14, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-14, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-14, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-14, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-14, groupId=test] Member consumer-test-14-6ea23b77-bc6c-4cb6-9605-1f1ffa2962bf sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-14, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-14, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-14 unregistered
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-18
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:33:06 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-18] Instantiated an idempotent producer.
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956386975
  2024-08-29 20:33:06 [ WARN] - org.apache.kafka.clients.NetworkClient -NetworkClient.java(1145) -[Producer clientId=producer-18] Error while fetching metadata with correlation id 1 : {yang.tests.error=LEADER_NOT_AVAILABLE}
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-18] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:06 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-18] ProducerId set to 110 with epoch 0
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-18] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-18 unregistered
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-19
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:33:07 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = false
	yang.json.fail.unknown.properties = true

  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-19] Instantiated an idempotent producer.
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956387116
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-19] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-19] ProducerId set to 111 with epoch 0
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-19] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-19 unregistered
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-15
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:33:07 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956387136
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-15, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-15, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-15, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-15, groupId=test] (Re-)joining group
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-15, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-15-13dbdf2e-b05c-4a3f-b132-8ec4c264ade2
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-15, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-15, groupId=test] (Re-)joining group
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-15, groupId=test] Successfully joined group with generation Generation{generationId=7, memberId='consumer-test-15-13dbdf2e-b05c-4a3f-b132-8ec4c264ade2', protocol='range'}
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-15, groupId=test] Finished assignment for group at generation 7: {consumer-test-15-13dbdf2e-b05c-4a3f-b132-8ec4c264ade2=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-15, groupId=test] Successfully synced group in generation Generation{generationId=7, memberId='consumer-test-15-13dbdf2e-b05c-4a3f-b132-8ec4c264ade2', protocol='range'}
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-15, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-15, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-15, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(1564) -[Consumer clientId=consumer-test-15, groupId=test] Seeking to offset 4 for partition yang.tests-0
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-15, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-15, groupId=test] Member consumer-test-15-13dbdf2e-b05c-4a3f-b132-8ec4c264ade2 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-15, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-15, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-15 unregistered
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-20
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:33:07 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-20] Instantiated an idempotent producer.
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956387176
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-20] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-20] ProducerId set to 112 with epoch 0
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-20] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-20 unregistered
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-16
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:33:07 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956387204
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-16, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-16, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-16, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-16, groupId=test] (Re-)joining group
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-16, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-16-c98ad163-1c65-4fb9-bae8-8ff342951d6e
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-16, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-16, groupId=test] (Re-)joining group
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-16, groupId=test] Successfully joined group with generation Generation{generationId=9, memberId='consumer-test-16-c98ad163-1c65-4fb9-bae8-8ff342951d6e', protocol='range'}
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-16, groupId=test] Finished assignment for group at generation 9: {consumer-test-16-c98ad163-1c65-4fb9-bae8-8ff342951d6e=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-16, groupId=test] Successfully synced group in generation Generation{generationId=9, memberId='consumer-test-16-c98ad163-1c65-4fb9-bae8-8ff342951d6e', protocol='range'}
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-16, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-16, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-16, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=4, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-16, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-16, groupId=test] Member consumer-test-16-c98ad163-1c65-4fb9-bae8-8ff342951d6e sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-16, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-16, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-16 unregistered
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-21
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:33:07 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-21] Instantiated an idempotent producer.
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956387763
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-21] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-21] ProducerId set to 113 with epoch 0
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-21] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-21 unregistered
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-17
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:33:07 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956387803
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-17, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-17, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-17, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-17, groupId=test] (Re-)joining group
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-17, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-17-b8e28646-7b13-4a12-a590-2a7ffaf6d109
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-17, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-17, groupId=test] (Re-)joining group
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-17, groupId=test] Successfully joined group with generation Generation{generationId=11, memberId='consumer-test-17-b8e28646-7b13-4a12-a590-2a7ffaf6d109', protocol='range'}
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-17, groupId=test] Finished assignment for group at generation 11: {consumer-test-17-b8e28646-7b13-4a12-a590-2a7ffaf6d109=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-17, groupId=test] Successfully synced group in generation Generation{generationId=11, memberId='consumer-test-17-b8e28646-7b13-4a12-a590-2a7ffaf6d109', protocol='range'}
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-17, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-17, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-17, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=5, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-17, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-17, groupId=test] Member consumer-test-17-b8e28646-7b13-4a12-a590-2a7ffaf6d109 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-17, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:33:07 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-17, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-17 unregistered
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-22
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:33:08 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-22] Instantiated an idempotent producer.
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956388367
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-22] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-22] ProducerId set to 114 with epoch 0
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-22] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-22 unregistered
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-18
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:33:08 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956388390
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-18, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-18, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-18, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-18, groupId=test] (Re-)joining group
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-18, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-18-2ff81776-c4f0-4964-8000-79ed254cc4e5
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-18, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-18, groupId=test] (Re-)joining group
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-18, groupId=test] Successfully joined group with generation Generation{generationId=13, memberId='consumer-test-18-2ff81776-c4f0-4964-8000-79ed254cc4e5', protocol='range'}
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-18, groupId=test] Finished assignment for group at generation 13: {consumer-test-18-2ff81776-c4f0-4964-8000-79ed254cc4e5=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-18, groupId=test] Successfully synced group in generation Generation{generationId=13, memberId='consumer-test-18-2ff81776-c4f0-4964-8000-79ed254cc4e5', protocol='range'}
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-18, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-18, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-18, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=6, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-18, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-18, groupId=test] Member consumer-test-18-2ff81776-c4f0-4964-8000-79ed254cc4e5 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-18, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-18, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-18 unregistered
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-23
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:33:08 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-23] Instantiated an idempotent producer.
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956388943
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-23] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-23] ProducerId set to 115 with epoch 0
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-23] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-23 unregistered
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-24
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:33:08 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-24] Instantiated an idempotent producer.
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956388971
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-24] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-24] ProducerId set to 116 with epoch 0
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-24] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-24 unregistered
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-19
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:33:08 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956388993
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-19, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-19, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-19, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-19, groupId=test] (Re-)joining group
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-19, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-19-0ccb9987-92b1-4cd1-9c37-64b4c35f6dfb
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-19, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:33:08 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-19, groupId=test] (Re-)joining group
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-19, groupId=test] Successfully joined group with generation Generation{generationId=15, memberId='consumer-test-19-0ccb9987-92b1-4cd1-9c37-64b4c35f6dfb', protocol='range'}
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-19, groupId=test] Finished assignment for group at generation 15: {consumer-test-19-0ccb9987-92b1-4cd1-9c37-64b4c35f6dfb=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-19, groupId=test] Successfully synced group in generation Generation{generationId=15, memberId='consumer-test-19-0ccb9987-92b1-4cd1-9c37-64b4c35f6dfb', protocol='range'}
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-19, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-19, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-19, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=7, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-19, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-19, groupId=test] Member consumer-test-19-0ccb9987-92b1-4cd1-9c37-64b4c35f6dfb sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-19, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-19, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-19 unregistered
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-25
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:33:09 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = false
	yang.json.fail.unknown.properties = true

  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-25] Instantiated an idempotent producer.
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956389548
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-25] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-25] ProducerId set to 117 with epoch 0
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-25] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-25 unregistered
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-20
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:33:09 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956389578
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-20, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-20, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-20, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-20, groupId=test] (Re-)joining group
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-20, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-20-8f0ed2c6-9be5-4367-ad24-06ed368b3252
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-20, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-20, groupId=test] (Re-)joining group
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-20, groupId=test] Successfully joined group with generation Generation{generationId=17, memberId='consumer-test-20-8f0ed2c6-9be5-4367-ad24-06ed368b3252', protocol='range'}
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-20, groupId=test] Finished assignment for group at generation 17: {consumer-test-20-8f0ed2c6-9be5-4367-ad24-06ed368b3252=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-20, groupId=test] Successfully synced group in generation Generation{generationId=17, memberId='consumer-test-20-8f0ed2c6-9be5-4367-ad24-06ed368b3252', protocol='range'}
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-20, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-20, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-20, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=8, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(1564) -[Consumer clientId=consumer-test-20, groupId=test] Seeking to offset 9 for partition yang.tests-0
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-20, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-20, groupId=test] Member consumer-test-20-8f0ed2c6-9be5-4367-ad24-06ed368b3252 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-20, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-20, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-20 unregistered
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-26
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:33:09 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-26] Instantiated an idempotent producer.
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956389615
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-26] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-26] ProducerId set to 118 with epoch 0
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-26] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-26 unregistered
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-21
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:33:09 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956389661
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-21, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-21, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-21, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-21, groupId=test] (Re-)joining group
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-21, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-21-32f7a311-7ed0-483b-b18f-71152ab1b6d8
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-21, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-21, groupId=test] (Re-)joining group
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-21, groupId=test] Successfully joined group with generation Generation{generationId=19, memberId='consumer-test-21-32f7a311-7ed0-483b-b18f-71152ab1b6d8', protocol='range'}
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-21, groupId=test] Finished assignment for group at generation 19: {consumer-test-21-32f7a311-7ed0-483b-b18f-71152ab1b6d8=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-21, groupId=test] Successfully synced group in generation Generation{generationId=19, memberId='consumer-test-21-32f7a311-7ed0-483b-b18f-71152ab1b6d8', protocol='range'}
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-21, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-21, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-21, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=9, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-21, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-21, groupId=test] Member consumer-test-21-32f7a311-7ed0-483b-b18f-71152ab1b6d8 sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-21, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:33:09 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-21, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-21 unregistered
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-27
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:33:10 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true

  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-27] Instantiated an idempotent producer.
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956390231
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-27] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-27] ProducerId set to 119 with epoch 0
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-27] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-27 unregistered
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.clients.producer.ProducerConfig -AbstractConfig.java(370) -ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-28
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializer

  2024-08-29 20:33:10 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaSerializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaSerializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = false
	yang.json.fail.unknown.properties = true

  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(580) -[Producer clientId=producer-28] Instantiated an idempotent producer.
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956390303
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Producer clientId=producer-28] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.clients.producer.internals.TransactionManager -TransactionManager.java(505) -[Producer clientId=producer-28] ProducerId set to 120 with epoch 0
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.clients.producer.KafkaProducer -KafkaProducer.java(1295) -[Producer clientId=producer-28] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.producer for producer-28 unregistered
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.clients.consumer.ConsumerConfig -AbstractConfig.java(370) -ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-22
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializer

  2024-08-29 20:33:10 [ INFO] - com.insa.kafka.serializers.yang.json.KafkaYangJsonSchemaDeserializerConfig -AbstractConfig.java(370) -KafkaYangJsonSchemaDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.RecordNameStrategy
	yang.json.fail.invalid.schema = true
	yang.json.fail.unknown.properties = true
	yang.json.key.type = class java.lang.Object
	yang.json.value.type = class java.lang.Object

  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(119) -Kafka version: 3.6.0
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(120) -Kafka commitId: 60e845626d8a465a
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(121) -Kafka startTimeMs: 1724956390335
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(912) -[Consumer clientId=consumer-test-22, groupId=test] Subscribed to topic(s): yang.tests
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.clients.Metadata -Metadata.java(287) -[Consumer clientId=consumer-test-22, groupId=test] Cluster ID: lj8hJBCWSWuMUV2S9KwuOg
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(906) -[Consumer clientId=consumer-test-22, groupId=test] Discovered group coordinator lenovo:9092 (id: 2147483647 rack: null)
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-22, groupId=test] (Re-)joining group
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-22, groupId=test] Request joining group due to: need to re-join with the given member-id: consumer-test-22-5222c373-6dc1-4d9d-a89d-3cc39aeab53e
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-22, groupId=test] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(576) -[Consumer clientId=consumer-test-22, groupId=test] (Re-)joining group
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(637) -[Consumer clientId=consumer-test-22, groupId=test] Successfully joined group with generation Generation{generationId=21, memberId='consumer-test-22-5222c373-6dc1-4d9d-a89d-3cc39aeab53e', protocol='range'}
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(717) -[Consumer clientId=consumer-test-22, groupId=test] Finished assignment for group at generation 21: {consumer-test-22-5222c373-6dc1-4d9d-a89d-3cc39aeab53e=Assignment(partitions=[yang.tests-0])}
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(812) -[Consumer clientId=consumer-test-22, groupId=test] Successfully synced group in generation Generation{generationId=21, memberId='consumer-test-22-5222c373-6dc1-4d9d-a89d-3cc39aeab53e', protocol='range'}
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(312) -[Consumer clientId=consumer-test-22, groupId=test] Notifying assignor about the new Assignment(partitions=[yang.tests-0])
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(324) -[Consumer clientId=consumer-test-22, groupId=test] Adding newly assigned partitions: yang.tests-0
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(980) -[Consumer clientId=consumer-test-22, groupId=test] Setting offset for partition yang.tests-0 to the committed offset FetchPosition{offset=10, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[lenovo:9092 (id: 0 rack: null)], epoch=0}}
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.clients.consumer.KafkaConsumer -KafkaConsumer.java(1564) -[Consumer clientId=consumer-test-22, groupId=test] Seeking to offset 11 for partition yang.tests-0
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -ConsumerCoordinator.java(343) -[Consumer clientId=consumer-test-22, groupId=test] Revoke previously assigned partitions yang.tests-0
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1133) -[Consumer clientId=consumer-test-22, groupId=test] Member consumer-test-22-5222c373-6dc1-4d9d-a89d-3cc39aeab53e sending LeaveGroup request to coordinator lenovo:9092 (id: 2147483647 rack: null) due to the consumer is being closed
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1025) -[Consumer clientId=consumer-test-22, groupId=test] Resetting generation and member id due to: consumer pro-actively leaving the group
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -AbstractCoordinator.java(1072) -[Consumer clientId=consumer-test-22, groupId=test] Request joining group due to: consumer pro-actively leaving the group
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(693) -Metrics scheduler closed
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(697) -Closing reporter org.apache.kafka.common.metrics.JmxReporter
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.common.metrics.Metrics -Metrics.java(703) -Metrics reporters closed
  2024-08-29 20:33:10 [ INFO] - org.apache.kafka.common.utils.AppInfoParser -AppInfoParser.java(83) -App info kafka.consumer for consumer-test-22 unregistered
  